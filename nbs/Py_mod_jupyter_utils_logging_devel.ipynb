{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging in Jupyter notebooks\n",
    "\n",
    "When evolving your notebook code, as it becomes more complex, your logging attempts will be forced to keepup: usually taking this path.\n",
    "\n",
    " - add `print` statements in your cells\n",
    " - tire of constant commenting/uncommenting the `prints`\n",
    " - research logging and learn that you can use python's loggers in notebooks! Yay!\n",
    " - research further and figure out that you can direct builting loggers for your imports as well.\n",
    " - Look at this verbose output üòñ!! Discover the power of visual separation with colors! \n",
    "\n",
    "This notebook demonstrates the above evolution. \n",
    "\n",
    "## Basic Logging\n",
    "\n",
    "The cell below shows how you'd setup basic logging. The `reload(logging)` is needed because of the singleton aspect of the config. In a jupyter environment _(and you might face this in your standalone scripts as well)_, something in the infrastructure already initializes the logger. Once that is done, subsequent calls to `logging.basicConfig` do not have any effect. This is where `reload` comes in: it clears the module state, thus allowing the subsequent `logging.basicConfig` call to take effect.\n",
    "\n",
    "> If you already have code with `basicConfig` **without** a `reload(logging)`, you can restart the notebook kernel for a new log_level to take effect: switch from `logging.DEBUG` to `logging.INFO` say.\n",
    "\n",
    "Further down this notebook there is code for \n",
    " - Color coding log-levels _(not important enough to make it into the lib but you could add it in your fork)_\n",
    " - Strategies to separate the clutter of the log from the actual output.\n",
    "\n",
    "üëá is going into the `jupyter_utils.py` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def setup_logging(level = logging.DEBUG):\n",
    "     \"\"\"\n",
    "     Supply one of logging.INFO|DEBUG|WARN|ERROR\n",
    "     \"\"\"\n",
    "     # Setup logging \n",
    "    # Note that module needs to be reloaded for our config to take as Jupyter \n",
    "    # already configures it: this makes all future configs no-ops unless a reload\n",
    "    # is performed.\n",
    "     from importlib import reload     \n",
    "     reload(logging)\n",
    "     logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', \n",
    "                    level=level, \n",
    "                    datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this is how you would introduce loggging into your code _(running in a jupyter cell)_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:20:10 DEBUG:My debug statement\n",
      "12:20:10 WARNING:My warning\n",
      "12:20:10 ERROR:My error\n"
     ]
    }
   ],
   "source": [
    "setup_logging(logging.DEBUG)\n",
    "\n",
    "def my_func():\n",
    "    # do stuff and then\n",
    "    logging.debug(\"My debug statement\")\n",
    "    logging.warning(\"My warning\")\n",
    "    logging.error(\"My error\")\n",
    "\n",
    "my_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ should show the three log statements you just printed out from the cell above.\n",
    "\n",
    "## Control logging from imports - OpenAI - Env vars\n",
    "\n",
    "With LLMs _(any python package for that matter)_, for instance, there are many times when you want visibility into low level decision making. Particularly those that might cause latency spikes, like HTTP response codes, hitting rate-limits and automatic retries. Surfacing their log traces will offer additional detail and hopefully enough to help. Many of these log statements will also improve discovery: when you suddenly see something that might be relevant. You then go off and research the API in more detail regarding that new thing.\n",
    "\n",
    "This section will demonstrate controlling `OpenAI` logging when using their LLM APIs. Their documentation shows that they use `OPENAI_LOG` environment variale to control their loggers. Other LLM vendors should work similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to log OpenAI's python library itself, also set the log level for this\n",
    "# normally, limit this to warning/error and keep your own logging at debug levels.\n",
    "# If this doesn't work right away, restart the kernel after changing the log-level\n",
    "import os\n",
    "os.environ[\"OPENAI_LOG\"]=\"debug\"\n",
    "\n",
    "import openai\n",
    "\n",
    "# Expects a OPENAI_API_KEY env var\n",
    "def get_completion(prompt, model=\"gpt-4o-mini\", temperature=0) -> str:\n",
    "    chat_history = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=chat_history,\n",
    "        temperature=temperature)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-01 12:35:18 - openai._base_client:482 - DEBUG] Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-303e8deb-ca6f-404d-b56b-fd3fbbb6c252', 'json_data': {'messages': [{'role': 'user', 'content': 'Why is the sky blue'}], 'model': 'gpt-4o-mini', 'temperature': 0}}\n",
      "[2025-06-01 12:35:18 - openai._base_client:965 - DEBUG] Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "[2025-06-01 12:35:22 - httpx:1025 - INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-06-01 12:35:22 - openai._base_client:1003 - DEBUG] HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Sun, 01 Jun 2025 19:35:22 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-uxl7oko9mdo17utucmetfrwn'), ('openai-processing-ms', '2661'), ('openai-version', '2020-10-01'), ('x-envoy-upstream-service-time', '2666'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '199993'), ('x-ratelimit-reset-requests', '8.64s'), ('x-ratelimit-reset-tokens', '2ms'), ('x-request-id', 'req_73990dd32d323049fc8dfd8898d2821a'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=a4BlQQOw7l1MuEXJTJRwA2nzAvrd710qBXEUQCOnQ2Y-1748806522-1.0.1.1-oAcW4TUEp5KUQuHlWCTWatdBrwTcXaBpUUMj7qQh3MLUupttx2f3AbzSzqOUcjLYteQKBstZ_mB64LeQoS4d0Tb44Dt0SoTRdiMBCooskvQ; path=/; expires=Sun, 01-Jun-25 20:05:22 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=Z46zrs1L8szME9iGFG_TMhRakaK_t1ULPb6AXCTk6qY-1748806522351-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '94912749492b2b4d-LAX'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "[2025-06-01 12:35:22 - openai._base_client:1011 - DEBUG] request_id: req_73990dd32d323049fc8dfd8898d2821a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue primarily due to a phenomenon called Rayleigh scattering. When sunlight enters the Earth's atmosphere, it is made up of different colors, each with varying wavelengths. Blue light has a shorter wavelength compared to other colors like red or yellow.\n",
      "\n",
      "As sunlight passes through the atmosphere, it collides with gas molecules and small particles. Because blue light is scattered in all directions more than other colors due to its shorter wavelength, we see a predominance of blue when we look up at the sky.\n",
      "\n",
      "During sunrise and sunset, the sky can appear red or orange because the sunlight has to pass through a greater thickness of the atmosphere. This longer path scatters the shorter blue wavelengths out of our line of sight, allowing the longer red wavelengths to dominate.\n"
     ]
    }
   ],
   "source": [
    "print(get_completion(\"Why is the sky blue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'openai' from '/home/vamsi/mambaforge/envs/ml-pip/lib/python3.12/site-packages/openai/__init__.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import logging\n",
    "import openai\n",
    "reload(logging)\n",
    "reload(openai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice two things üëÜ\n",
    "\n",
    " - Those logs üòç\n",
    " - Such log! much noise! Where's my actual output at üòü\n",
    "\n",
    "We'll get to decluttering the visual a bit later.\n",
    "\n",
    "## Control logging when no env vars are available\n",
    "\n",
    "> This is useful when you want to change things other than log levels as well. See the formatter example below\n",
    "\n",
    "It's nice that OpenAI provides the `OPENAL_LOG` env var: very easy to control that. However, in cases where you don't have access to such a variable, you can manipulate the logger directly: you just have to get to the logger in use.\n",
    "\n",
    "### Examine the loggers avaiable.\n",
    "\n",
    "Note that the loggers are usually initialized at the module level on first use. So you'll likely need to exercise some code to get to them. ALl of this just to get the logger by name. Once you know the name, it usually doesn't change unless some major revision occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['httpx', 'rich', 'openai', 'openai._legacy_response', 'openai._response', 'openai._base_client', 'openai.resources.beta.realtime.realtime', 'openai.resources.beta.realtime', 'openai.resources.beta', 'openai.resources', 'openai.audio.transcriptions', 'openai.audio', 'openai.resources.uploads.uploads', 'openai.resources.uploads', 'httpcore.http11', 'httpcore', 'httpcore.connection', 'httpcore.proxy']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " * openai\n",
       " * openai._legacy_response\n",
       " * openai._response\n",
       " * openai._base_client\n",
       " * openai.resources.beta.realtime.realtime\n",
       " * openai.resources.beta.realtime\n",
       " * openai.resources.beta\n",
       " * openai.resources\n",
       " * openai.audio.transcriptions\n",
       " * openai.audio\n",
       " * openai.resources.uploads.uploads\n",
       " * openai.resources.uploads"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Use this to explore available loggers\n",
    "# If there is a logger and you are not provided an env-var top control log level, \n",
    "# you can directly call logger.setLevel(Logging.DEBUG) to collect logs.\n",
    "def get_available_loggers():\n",
    "    return [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "\n",
    "\n",
    "# To see them all.\n",
    "all_loggers = get_available_loggers()\n",
    "print([l.name for l in all_loggers])\n",
    "\n",
    "# Say we are interested only in openai\n",
    "# Long list, I want this formatted nicely. Markdown formatting is easy enough to generate\n",
    "# compared to HTML\n",
    "openai_logger_names = [l.name for l in get_available_loggers() if 'openai' in l.name]\n",
    "display(Markdown(\n",
    "    \"\\n\".join([f\" * {item}\" for item in openai_logger_names])\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the log level directly on the selected logger\n",
    "\n",
    "The query above shows a logger called `openai`: likely the root logger with individual sub-modules having child loggers. This is how one normally does things so that while testing a sub-module, you can set it's log-level to `INFO` say while reducing the noise down to `ERROR` for everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Say we want to customize the 'openai` logger. It likely is inherited by the openai.xxx child-loggers\n",
    "# but not sure if they copy the parent settings (and thus break the link) on reference it. Basically, \n",
    "# you may have to customize the individual child loggers if changes to root-logger customization \n",
    "# does not have any affect.\n",
    "oai_logger = list(filter(lambda l: l.name == \"openai\", all_loggers))[0]\n",
    "\n",
    "# Since we already have all_loggers, I am using a filter on it.\n",
    "# However, once you know the name, you can also use\n",
    "# üëâ  oai_logger = logging.root.manager.loggerDict.get('openai')\n",
    "#-----------------------\n",
    "# Set the level directly\n",
    "oai_logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Distinguishing log output from your cell output \n",
    "\n",
    "The main problem _(as illustrated in a previous call to OpenAI's completion API)_ is that of noise. Simply too much stuff and it takes attention away from the output you really care about. Thankfully, there are several easy solutions. The simplest would be to make use of Jupyter notebook's builtin markdown renderer _(also immensely useful when you have LLM output in markdown or want to convert something to markdown for some easy formatting)_. \n",
    "\n",
    "> Definitely pays to know your markdown. Mich simpler and less verbose than HTML.\n",
    ">\n",
    "> There are advanced ways to manipulate IPython displays using ipywidgets and display(id). Explore along those lines if organizing logs into a separate cell turns out to be important for your use cases.\n",
    "\n",
    "### Use a markdown separator\n",
    "\n",
    "Simply throw in a markdown separator add/or a markdown section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def markdown_separator(section_name = None):\n",
    "    if section_name:\n",
    "        display(Markdown(f\"----\\n### {section_name}\\n\"))\n",
    "    else:\n",
    "        display(Markdown(f\"----\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:00:53 DEBUG:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Why is the sky blue'}], 'model': 'gpt-4o-mini', 'temperature': 0}}\n",
      "08:00:53 DEBUG:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "08:00:53 DEBUG:close.started\n",
      "08:00:53 DEBUG:close.complete\n",
      "08:00:53 DEBUG:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "08:00:53 DEBUG:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f9fb520f460>\n",
      "08:00:53 DEBUG:start_tls.started ssl_context=<ssl.SSLContext object at 0x7f9fb5256e70> server_hostname='api.openai.com' timeout=5.0\n",
      "08:00:53 DEBUG:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f9fb520f130>\n",
      "08:00:53 DEBUG:send_request_headers.started request=<Request [b'POST']>\n",
      "08:00:53 DEBUG:send_request_headers.complete\n",
      "08:00:53 DEBUG:send_request_body.started request=<Request [b'POST']>\n",
      "08:00:53 DEBUG:send_request_body.complete\n",
      "08:00:53 DEBUG:receive_response_headers.started request=<Request [b'POST']>\n",
      "08:00:56 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 05 Mar 2025 16:00:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-uxl7oko9mdo17utucmetfrwn'), (b'openai-processing-ms', b'2857'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199978'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'6ms'), (b'x-request-id', b'req_2588771bb8c89d359131e92b4420abcf'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'91bad4343da82b5d-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "08:00:56 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "08:00:56 DEBUG:receive_response_body.started request=<Request [b'POST']>\n",
      "08:00:56 DEBUG:receive_response_body.complete\n",
      "08:00:56 DEBUG:response_closed.started\n",
      "08:00:56 DEBUG:response_closed.complete\n",
      "08:00:56 DEBUG:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 05 Mar 2025 16:00:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-uxl7oko9mdo17utucmetfrwn', 'openai-processing-ms': '2857', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199978', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '6ms', 'x-request-id': 'req_2588771bb8c89d359131e92b4420abcf', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '91bad4343da82b5d-LAX', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "08:00:56 DEBUG:request_id: req_2588771bb8c89d359131e92b4420abcf\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "----\n",
       "# OpenAI Response\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The sky appears blue primarily due to a phenomenon called Rayleigh scattering. When sunlight enters the Earth's atmosphere, it is made up of different colors, each with varying wavelengths. Blue light has a shorter wavelength compared to other colors like red or yellow.\n",
       "\n",
       "As sunlight passes through the atmosphere, it collides with gas molecules and small particles. Because blue light is scattered in all directions more than other colors due to its shorter wavelength, we see a predominance of blue when we look up at the sky.\n",
       "\n",
       "During sunrise and sunset, the sun is lower on the horizon, and its light has to pass through a greater thickness of the atmosphere. This increased distance scatters the shorter blue wavelengths out of our line of sight, allowing the longer wavelengths like red and orange to dominate, which is why the sky can appear red or orange during those times."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run to completion so all logs are printed out\n",
    "res = get_completion(\"Why is the sky blue\")\n",
    "\n",
    "# print separator\n",
    "markdown_separator(\"OpenAI Response\")\n",
    "\n",
    "# print your result\n",
    "# The use of markdown here formats it into the space available.\n",
    "# Otw you'll get horizontal scrollbars\n",
    "display(Markdown(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color the cell output\n",
    "\n",
    "Take advantage of the `IPython.display.Html` object and render any HTML that you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For displaying HTML and Markdown responses from ChatGPT\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Enhance with more Html (fg-color, font, etc) as needed but title is usually a good starting point.\n",
    "def colorBox(txt, title=None):\n",
    "    if title is not None:\n",
    "        txt = f\"<b>{title}</b><br><hr><br>{txt}\"\n",
    "\n",
    "    display(HTML(f\"<div style='border-radius:15px;padding:15px;background-color:pink;color:black;'>{txt}</div>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-01 12:47:14 - openai._base_client:482 - DEBUG] Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-96af53e8-f605-43d9-9488-cd668e69eb1a', 'json_data': {'messages': [{'role': 'user', 'content': 'Why is the sky blue'}], 'model': 'gpt-4o-mini', 'temperature': 0}}\n",
      "[2025-06-01 12:47:14 - openai._base_client:965 - DEBUG] Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "[2025-06-01 12:47:17 - httpx:1025 - INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-06-01 12:47:17 - openai._base_client:1003 - DEBUG] HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Sun, 01 Jun 2025 19:47:17 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-uxl7oko9mdo17utucmetfrwn', 'openai-processing-ms': '3222', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '3226', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9998', 'x-ratelimit-remaining-tokens': '199992', 'x-ratelimit-reset-requests': '9.787s', 'x-ratelimit-reset-tokens': '2ms', 'x-request-id': 'req_e4b66b75a8b031eaec0c48afad848897', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '949138c0dc3908e6-LAX', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "[2025-06-01 12:47:17 - openai._base_client:1011 - DEBUG] request_id: req_e4b66b75a8b031eaec0c48afad848897\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;background-color:pink;color:black;'><b>OpenAI Response</b><br><hr><br>The sky appears blue primarily due to a phenomenon called Rayleigh scattering. When sunlight enters the Earth's atmosphere, it is made up of different colors, each with varying wavelengths. Blue light has a shorter wavelength compared to other colors like red or yellow.\n",
       "\n",
       "As sunlight passes through the atmosphere, it collides with gas molecules and small particles. Because blue light is scattered in all directions more than other colors due to its shorter wavelength, we see a predominance of blue when we look up at the sky.\n",
       "\n",
       "During sunrise and sunset, the sun's light has to pass through a greater thickness of the atmosphere, which scatters the shorter blue wavelengths out of our line of sight and allows the longer wavelengths, like red and orange, to dominate, resulting in the beautiful colors we see at those times.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Run to completion so all logs are printed out\n",
    "res = get_completion(\"Why is the sky blue\")\n",
    "\n",
    "# print your result\n",
    "# The use of markdown here formats it into the space available.\n",
    "# Otw you'll get horizontal scrollbars\n",
    "colorBox(res, title=\"OpenAI Response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color the log output to quickly zero in on errors\n",
    "\n",
    "Code below mostly copied from https://stackoverflow.com/questions/68807282/rich-logging-output-in-jupyter-ipython-notebook üôè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "class DisplayHandler(logging.Handler):\n",
    "    def emit(self, record):\n",
    "        message = self.format(record)\n",
    "        display(message)\n",
    "\n",
    "class HTMLFormatter(logging.Formatter):\n",
    "    level_colors = {\n",
    "        logging.DEBUG: 'lightblue',\n",
    "        logging.INFO: 'dodgerblue',\n",
    "        logging.WARNING: 'goldenrod',\n",
    "        logging.ERROR: 'crimson',\n",
    "        logging.CRITICAL: 'firebrick'\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            '<span style=\"font-weight: bold; color: green\">{asctime}</span> '\n",
    "            '[<span style=\"font-weight: bold; color: {levelcolor}\">{levelname}</span>] '\n",
    "            '{message}',\n",
    "            style='{'\n",
    "        )\n",
    "    \n",
    "    def format(self, record):\n",
    "        record.levelcolor = self.level_colors.get(record.levelno, 'black')\n",
    "        return HTML(super().format(record))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the cells above reveals a logger named `openai`\n",
    "# with some trial and error, turns out that some of these log statements come from `httpx`\n",
    "# Lets target that and change it's formatter to the above colorful one\n",
    "handler = DisplayHandler()\n",
    "handler.setFormatter(HTMLFormatter())\n",
    "\n",
    "for nm in ['openai', 'httpx']:\n",
    "    lg = logging.root.manager.loggerDict.get(nm)\n",
    "    lg.addHandler(handler)\n",
    "    lg.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"font-weight: bold; color: green\">2025-03-05 12:49:04,911</span> [<span style=\"font-weight: bold; color: lightblue\">DEBUG</span>] Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Why is the sky blue'}], 'model': 'gpt-4o-mini', 'temperature': 0}}"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-weight: bold; color: green\">2025-03-05 12:49:04,911</span> [<span style=\"font-weight: bold; color: lightblue\">DEBUG</span>] Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Why is the sky blue'}], 'model': 'gpt-4o-mini', 'temperature': 0}}"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:49:04 DEBUG:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Why is the sky blue'}], 'model': 'gpt-4o-mini', 'temperature': 0}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-weight: bold; color: green\">2025-03-05 12:49:04,915</span> [<span style=\"font-weight: bold; color: lightblue\">DEBUG</span>] Sending HTTP Request: POST https://api.openai.com/v1/chat/completions"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-weight: bold; color: green\">2025-03-05 12:49:04,915</span> [<span style=\"font-weight: bold; color: lightblue\">DEBUG</span>] Sending HTTP Request: POST https://api.openai.com/v1/chat/completions"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:49:04 DEBUG:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "12:49:04 DEBUG:close.started\n",
      "12:49:04 DEBUG:close.complete\n",
      "12:49:04 DEBUG:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "12:49:04 DEBUG:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f9fb51efa70>\n",
      "12:49:04 DEBUG:start_tls.started ssl_context=<ssl.SSLContext object at 0x7f9fb5256e70> server_hostname='api.openai.com' timeout=5.0\n",
      "12:49:05 DEBUG:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f9fcca99d00>\n",
      "12:49:05 DEBUG:send_request_headers.started request=<Request [b'POST']>\n",
      "12:49:05 DEBUG:send_request_headers.complete\n",
      "12:49:05 DEBUG:send_request_body.started request=<Request [b'POST']>\n",
      "12:49:05 DEBUG:send_request_body.complete\n",
      "12:49:05 DEBUG:receive_response_headers.started request=<Request [b'POST']>\n",
      "12:49:06 DEBUG:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 05 Mar 2025 20:49:07 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-uxl7oko9mdo17utucmetfrwn'), (b'openai-processing-ms', b'1427'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199977'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'6ms'), (b'x-request-id', b'req_f9715aba22a756b1da8e4aaf0de6d9a9'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'91bc7a5e3f842a85-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-weight: bold; color: green\">2025-03-05 12:49:06,521</span> [<span style=\"font-weight: bold; color: dodgerblue\">INFO</span>] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:49:06 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "12:49:06 DEBUG:receive_response_body.started request=<Request [b'POST']>\n",
      "12:49:06 DEBUG:receive_response_body.complete\n",
      "12:49:06 DEBUG:response_closed.started\n",
      "12:49:06 DEBUG:response_closed.complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-weight: bold; color: green\">2025-03-05 12:49:06,531</span> [<span style=\"font-weight: bold; color: lightblue\">DEBUG</span>] HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 05 Mar 2025 20:49:07 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-uxl7oko9mdo17utucmetfrwn', 'openai-processing-ms': '1427', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199977', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '6ms', 'x-request-id': 'req_f9715aba22a756b1da8e4aaf0de6d9a9', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '91bc7a5e3f842a85-LAX', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-weight: bold; color: green\">2025-03-05 12:49:06,531</span> [<span style=\"font-weight: bold; color: lightblue\">DEBUG</span>] HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 05 Mar 2025 20:49:07 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-uxl7oko9mdo17utucmetfrwn', 'openai-processing-ms': '1427', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199977', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '6ms', 'x-request-id': 'req_f9715aba22a756b1da8e4aaf0de6d9a9', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '91bc7a5e3f842a85-LAX', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:49:06 DEBUG:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 05 Mar 2025 20:49:07 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-uxl7oko9mdo17utucmetfrwn', 'openai-processing-ms': '1427', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199977', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '6ms', 'x-request-id': 'req_f9715aba22a756b1da8e4aaf0de6d9a9', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '91bc7a5e3f842a85-LAX', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-weight: bold; color: green\">2025-03-05 12:49:06,534</span> [<span style=\"font-weight: bold; color: lightblue\">DEBUG</span>] request_id: req_f9715aba22a756b1da8e4aaf0de6d9a9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-weight: bold; color: green\">2025-03-05 12:49:06,534</span> [<span style=\"font-weight: bold; color: lightblue\">DEBUG</span>] request_id: req_f9715aba22a756b1da8e4aaf0de6d9a9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:49:06 DEBUG:request_id: req_f9715aba22a756b1da8e4aaf0de6d9a9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;background-color:pink;color:black;'><b>OpenAI Response</b><br><hr><br>The sky appears blue primarily due to a phenomenon called Rayleigh scattering. When sunlight enters the Earth's atmosphere, it is made up of different colors, each with varying wavelengths. Blue light has a shorter wavelength compared to other colors like red or yellow.\n",
       "\n",
       "As sunlight passes through the atmosphere, it collides with gas molecules and small particles. Because blue light is scattered in all directions more than other colors due to its shorter wavelength, we see a predominance of blue when we look up at the sky.\n",
       "\n",
       "During sunrise and sunset, the sun's light has to pass through a thicker layer of the atmosphere, which scatters the shorter blue wavelengths out of our line of sight and allows the longer wavelengths, like red and orange, to dominate, giving the sky those warm colors.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run to completion so all logs are printed out\n",
    "res = get_completion(\"Why is the sky blue\")\n",
    "\n",
    "# print your result\n",
    "# The use of markdown here formats it into the space available.\n",
    "# Otw you'll get horizontal scrollbars\n",
    "colorBox(res, title=\"OpenAI Response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above colors some of the log outputs. If you care to, you could experiment with changing root logger formatting or expand it to all the loggers available _(from `[logging.getLogger(name) for name in logging.root.manager.loggerDict]`)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
