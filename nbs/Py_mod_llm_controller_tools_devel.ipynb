{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of Tool Calling functionality and py module - OpenAI\n",
    "\n",
    "> The code developed here is finally copied to `llm/controller/tool_calling.py` and available as the `llm.controller.tool_calling` module\n",
    "\n",
    "This notebook develops the tool-calling functionality on top of plain one-shot prompting. While OpenAI is chosen as the LLM provider, the code should work for most everyone with hopefully minor modifcations.\n",
    "\n",
    "While `tool` and `using tools in LLM interactions` belong together, I am splitting the development into two notebooks since a single notbook covering both aspects was getting too large.\n",
    " - The [tool infrastructure](./Py_mod_llm_tools_devel.ipynb) that helps define the tools, generating their schemas and such.\n",
    " - This notebook which details the higher-level loop/driver _(shown below)_ that takes in a bunch of tool definitions and drives a chat to completion.\n",
    "\n",
    " ![](../img/tool_calling_protocol.png)\n",
    "\n",
    "The hello-world of tool-calling, _get_weather_ is described in the official [OpenAI docs](https://platform.openai.com/docs/guides/function-calling). The OpenAI implementation _(atleast when it came out in late 2023)_ did not specify how one would build the json spec.Our implementation here demonstrates the use of pydantic classes to\n",
    " - Automate generation of the JSON schema required for tools\n",
    " - Automate deserialization of the JSON args supplied by OpenAI\n",
    " - Simplify implementation of tooling for ReAct and any other LLM use case.\n",
    " - Similar infra is also used for obtaining structured outputs from OpenAI and others.\n",
    "\n",
    "![](../img/tool_calling_protocol_impl_details.png)\n",
    "\n",
    "There are two scenarios demonstrated here\n",
    " - `What is the weather in XX` which validates basic tool calling\n",
    " - `Increase Temperature by YY` which tests if the LLM can first call `get_temperature`, do some math and then call `set_temperature`.\n",
    "\n",
    "## Setup Basic Environment\n",
    "\n",
    "> Normally, these would go into a python module and I would include it in my \n",
    "module search path. However, when executed in colab directly from github, It requires that I put my lib code in a repo, clone said repo into colab environment and then add it to my path. Will deal with that later!\n",
    "\n",
    " - Logging\n",
    " - Colab environment\n",
    " - OpenAI functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow Colab or VSCore/Normal Jupyter environments\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# The default relative path when running the notebook from a cloned repo.\n",
    "LIB_PATH = Path(\"../lib\")\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print(\"👉 Setting up for Colab\")\n",
    "    # This will create a py-llm dir at the same level as this notebook\n",
    "    # Refer to the lib in there using `./py-llm/lib` as opposed to the \n",
    "    # relative `../lib` when we are running straight from the py-llm/nbs \n",
    "    # directory in VScode.\n",
    "    if not os.path.isdir(\"./py-llm\"):\n",
    "        print(\"Cloning git repo into ./py-llm\")\n",
    "        !git clone -b 3_llm_tools_and_support https://github.com/vamsi-juvvi/py-llm.git\n",
    "        LIB_PATH = Path(\"./py-llm/lib\")\n",
    "    else:\n",
    "        print(\"./py-llm exists. Not cloning\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append to sys.path directly\n",
    "# Make sure to `str(Path)`\n",
    "# - The resolve() converts relative to absolute. \n",
    "# - If you use ~ for HOME, use `Path.expand_user()`\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "sys.path.append(str(LIB_PATH.resolve()))\n",
    "\n",
    "from py_llm.util import jupyter_util\n",
    "from py_llm.util.jupyter_util import TextAlign\n",
    "from py_llm.util.jupyter_util import DisplayHTML as DH\n",
    "from py_llm.util.jupyter_util import DisplayMarkdown as DM\n",
    "from py_llm.util.jupyter_util import ColabEnv\n",
    "\n",
    "from py_llm.llm import openai_util as oai\n",
    "from py_llm.llm.tools import Tool, ToolCollection\n",
    "\n",
    "# Init jupyter env\n",
    "jupyter_util.setup_logging(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for use in Colab or when the package is missing\n",
    "#!pip install -qy openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# If you want to log OpenAI's python library itself, also set the log level for this\n",
    "# normally, limit this to warning/error and keep your own logging at debug levels.\n",
    "# If this doesn't work right away, restart the kernel after changing the log-level\n",
    "os.environ[\"OPENAI_LOG\"]=\"error\"\n",
    "\n",
    "# Finally ensure you have the OpenAI key.\n",
    "openai.api_key = ColabEnv.colab_keyval_or_env(\"OPENAI_API_KEY\")\n",
    "assert(openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use llm.tools module for our tools\n",
    "\n",
    "↪ The development notes for tool-schema creation etc have been moved to [Py_mod_llm_tools_devel.ipynb](./Py_mod_llm_tools_devel.ipynb) where further evolution notes are also maintained. The code from that notebook was copied into the `llm.tools` module and that is what will be used in the rest of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The chat loop with tools involved\n",
    "\n",
    "Unlike a one-shot prompt, when tools are used:\n",
    "\n",
    " - the LLM can respond with one or more `tool call`s instead of an `assistant response`. \n",
    " - We need to evaluate all the tool calls and respond. \n",
    " - This is continued till the LLM responsds with an assistant response \n",
    " - Then we are done.\n",
    "\n",
    "![](../img/toolcollection_calling_protocol_impl_details.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat_loop(prompt:str, tools : ToolCollection):\n",
    "    \"\"\"\n",
    "    Runs a chat loop with an initial prompt and supplied tools\n",
    "    Resolves all tool_calls made till a final assistant response is provided\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    chat_history = [\n",
    "        {\n",
    "            \"role\" : \"system\",\n",
    "            \"content\" : \"You are a helpful assistant that uses the supplied tools to respond to the user's questions.\"\n",
    "        }]\n",
    "\n",
    "    tool_schemas = tools.get_schemas()\n",
    "\n",
    "    # Run the loop\n",
    "    msgs = [{\n",
    "        \"role\":\"user\", \n",
    "        \"content\": prompt}]\n",
    "\n",
    "    DH.color_box(prompt, title=\"Prompt\", bg=\"yellow\")\n",
    "    \n",
    "    while len(msgs):\n",
    "        chat_history.extend(msgs)\n",
    "        msgs = []\n",
    "\n",
    "        response = oai.get_response(\n",
    "            chat_history=chat_history,\n",
    "            tools = tool_schemas)\n",
    "\n",
    "        # tool-call\n",
    "        # Note: The OpenAI example is outdated\n",
    "        # tool_calls is not longer a JSON object but an array of \n",
    "        # `ChatCompletionMessageToolCall` objects\n",
    "        if response.choices[0].message.tool_calls:\n",
    "\n",
    "            # The tool-call set needs to be added back to the chat_history\n",
    "            msgs.append(response.choices[0].message)\n",
    "\n",
    "            # Process all the tool calls\n",
    "            for tool_call in response.choices[0].message.tool_calls:\n",
    "                logging.debug(f\"Executing tool_call: {tool_call}\")            \n",
    "\n",
    "                DH.color_box(f\"{tool_call.function.name}({tool_call.function.arguments})\", \n",
    "                             title=\"LLM Tool Call\",\n",
    "                             bg=\"lightgreen\",\n",
    "                             align = TextAlign.RIGHT)\n",
    "                tool_result = tools.exec_tool(\n",
    "                    tool_call.function.name,\n",
    "                    tool_call.function.arguments)\n",
    "                assert(isinstance(tool_result, str))\n",
    "\n",
    "                # along with it's response. The response will be linked to the tool_call's \n",
    "                # via the ID        \n",
    "                msgs.append({\n",
    "                    \"role\" : \"tool\",\n",
    "                    \"tool_call_id\" : tool_call.id,\n",
    "                    \"content\"      : tool_result\n",
    "                })\n",
    "        else:\n",
    "            # Assistant response\n",
    "            chat_response = response.choices[0].message.content\n",
    "            DH.color_box(chat_response, title=\"Final LLM Response\", align=TextAlign.LEFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement OpenAI's get_weather example\n",
    "\n",
    "See https://platform.openai.com/docs/guides/function-calling. I an including it inline below\n",
    "\n",
    "----\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current temperature for a given location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City and country e.g. Bogotá, Colombia\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\n",
    "                \"location\"\n",
    "            ],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the weather like in Paris today?\"}],\n",
    "    tools=tools\n",
    ")\n",
    "```\n",
    "\n",
    "My goal is to automate the creation of the json _(the `tools` variable above)_ given a function. The previously created `getToolJsonSchema` handles the creation of the schema given a function: the imposed limitation is that the function, if it has arguments, is limited to just 1 and it must be a pydantic class.\n",
    "\n",
    "> 👉 Note that in a production scenario, one will enfore that all the fields and the function itself will have a reasonable descriptions. We need good natural language descriptions to allow the LLM to decide which tool to call.\n",
    "\n",
    "### Create a datamodel for weather args and a function that uses it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:54:45 DEBUG:Tool : get_weather, Initialization\n",
      "10:54:45 DEBUG:Tool : get_weather, Schema=\n",
      "{\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "        \"name\": \"get_weather\",\n",
      "        \"description\": \"Get current temperature for a given location.\",\n",
      "        \"strict\": true,\n",
      "        \"parameters\": {\n",
      "            \"properties\": {\n",
      "                \"location\": {\n",
      "                    \"description\": \"City and country e.g. San Jose, USA\",\n",
      "                    \"title\": \"Location\",\n",
      "                    \"type\": \"string\"\n",
      "                }\n",
      "            },\n",
      "            \"required\": [\n",
      "                \"location\"\n",
      "            ],\n",
      "            \"title\": \"GetWeather\",\n",
      "            \"type\": \"object\",\n",
      "            \"additionalProperties\": false\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GetWeather(BaseModel):        \n",
    "    location : str = Field(description=\"City and country e.g. San Jose, USA\")\n",
    "\n",
    "def get_weather(args: GetWeather) -> float:\n",
    "    \"\"\"\n",
    "    Get current temperature for a given location.\n",
    "    \"\"\"\n",
    "    retval = \"10\"\n",
    "    logging.debug(f\"get_weather called with {args}. Returning hardcoded value {retval}\")\n",
    "    return retval\n",
    "\n",
    "# Register in the tool dictionary\n",
    "# Note that one could evolve this further to make the tool_dict the single \n",
    "# source and use it to the tool descriptions as well.\n",
    "gw_tools = ToolCollection()\n",
    "gw_tools.register_tool(Tool(get_weather))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Test get_weather call manually"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### The autogenerated schema for `get_weather`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "[\n",
       "    {\n",
       "        \"type\": \"function\",\n",
       "        \"function\": {\n",
       "            \"name\": \"get_weather\",\n",
       "            \"description\": \"Get current temperature for a given location.\",\n",
       "            \"strict\": true,\n",
       "            \"parameters\": {\n",
       "                \"properties\": {\n",
       "                    \"location\": {\n",
       "                        \"description\": \"City and country e.g. San Jose, USA\",\n",
       "                        \"title\": \"Location\",\n",
       "                        \"type\": \"string\"\n",
       "                    }\n",
       "                },\n",
       "                \"required\": [\n",
       "                    \"location\"\n",
       "                ],\n",
       "                \"title\": \"GetWeather\",\n",
       "                \"type\": \"object\",\n",
       "                \"additionalProperties\": false\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Executing call throught ToolCollection API"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:54:46 DEBUG:Executing tool: get_weather\n",
      "10:54:46 DEBUG:Attempting to deserialize {\"location\":\"Paris, France\"} for tool: get_weather\n",
      "10:54:46 DEBUG:✔️ deserialized to location='Paris, France'. Calling function\n",
      "10:54:46 DEBUG:get_weather called with location='Paris, France'. Returning hardcoded value 10\n",
      "10:54:46 DEBUG:✔️ function returned 10\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```None\n",
       "10\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test schema generation and execution!\n",
    "if 1:\n",
    "        DM.h(\"Test get_weather call manually\", title_level=2)\n",
    "\n",
    "        # Verify that the JSON we get from out `get_weather` function matches the \n",
    "        # raw JSON used in the OpenAI example\n",
    "        DM.h(\"The autogenerated schema for `get_weather`\", title_level=3)\n",
    "        DM.json(gw_tools.get_schemas())\n",
    "\n",
    "        # Test using the OpenAI example's serialized JSON\n",
    "        DM.h(\"Executing call throught ToolCollection API\", title_level=3)\n",
    "        DM.code(gw_tools.exec_tool(\"get_weather\", \"{\\\"location\\\":\\\"Paris, France\\\"}\"))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Test get_weather via run_chat_loop ⇔ LLM "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;text-align:justify;background-color:yellow;color:black;'><b>Prompt</b><br><hr><br>What is the weather in San Jose, USA?</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:55:23 DEBUG:Executing tool_call: ChatCompletionMessageToolCall(id='call_LHz0TAGCtkhyVhnzxd4EfG49', function=Function(arguments='{\"location\":\"San Jose, USA\"}', name='get_weather'), type='function')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;margin-left:100px;text-align:right;background-color:lightgreen;color:black;'><b>LLM Tool Call</b><br><hr><br>get_weather({\"location\":\"San Jose, USA\"})</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:55:23 DEBUG:Executing tool: get_weather\n",
      "10:55:23 DEBUG:Attempting to deserialize {\"location\":\"San Jose, USA\"} for tool: get_weather\n",
      "10:55:23 DEBUG:✔️ deserialized to location='San Jose, USA'. Calling function\n",
      "10:55:23 DEBUG:get_weather called with location='San Jose, USA'. Returning hardcoded value 10\n",
      "10:55:23 DEBUG:✔️ function returned 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;margin-right:100px;text-align:left;background-color:pink;color:black;'><b>Final LLM Response</b><br><hr><br>The current temperature in San Jose, USA is 10°C.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 1:\n",
    "        DM.h(\"Test get_weather via run_chat_loop ⇔ LLM \", title_level=2)        \n",
    "\n",
    "        run_chat_loop(\n",
    "            prompt=\"What is the weather in San Jose, USA?\",\n",
    "            tools = gw_tools\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:56:46 DEBUG:Tool : set_thermostat_temperature, Initialization\n",
      "10:56:46 DEBUG:Tool : set_thermostat_temperature, Schema=\n",
      "{\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "        \"name\": \"set_thermostat_temperature\",\n",
      "        \"description\": \"Sets the current temperature for a given location.\\\"\",\n",
      "        \"strict\": true,\n",
      "        \"parameters\": {\n",
      "            \"properties\": {\n",
      "                \"temp\": {\n",
      "                    \"description\": \"The temperature value in Fahrenheit to set the thermostat to\",\n",
      "                    \"title\": \"Temp\",\n",
      "                    \"type\": \"number\"\n",
      "                }\n",
      "            },\n",
      "            \"required\": [\n",
      "                \"temp\"\n",
      "            ],\n",
      "            \"title\": \"SetTemperature\",\n",
      "            \"type\": \"object\",\n",
      "            \"additionalProperties\": false\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "10:56:46 DEBUG:Tool : get_thermostat_temperature, Initialization\n",
      "10:56:46 DEBUG:Tool : get_thermostat_temperature, Schema=\n",
      "{\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "        \"name\": \"get_thermostat_temperature\",\n",
      "        \"description\": \"Returns the current temperature setting of the thermostat.\\\"\",\n",
      "        \"strict\": true,\n",
      "        \"parameters\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {},\n",
      "            \"required\": [],\n",
      "            \"additionalProperties\": false\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class SetTemperature(BaseModel):        \n",
    "    temp : float = Field(description=\"The temperature value in Fahrenheit to set the thermostat to\")\n",
    "\n",
    "def set_thermostat_temperature(args: SetTemperature) -> str:\n",
    "    \"\"\"\n",
    "    Sets the current temperature for a given location.\"\n",
    "    \"\"\"\n",
    "    logging.debug(f\"set_thermostat_temperature called with {args}\")\n",
    "    return \"\"\n",
    "\n",
    "def get_thermostat_temperature() -> str:\n",
    "    \"\"\"\n",
    "    Returns the current temperature setting of the thermostat.\"\n",
    "    \"\"\"\n",
    "    retval = \"60\"\n",
    "    logging.debug(f\"get_thermostat_temperature called. Returning hardcoded {retval}\")\n",
    "    return retval\n",
    "\n",
    "# Register in the tool dictionary\n",
    "iot_tools = ToolCollection()\n",
    "iot_tools.register_tool(Tool(set_thermostat_temperature))\n",
    "iot_tools.register_tool(Tool(get_thermostat_temperature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;text-align:justify;background-color:yellow;color:black;'><b>Prompt</b><br><hr><br>Increase the temperature by 10 degrees</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:56:50 DEBUG:Executing tool_call: ChatCompletionMessageToolCall(id='call_oRYvzGUQOQU5NGHmh2e6AlTf', function=Function(arguments='{}', name='get_thermostat_temperature'), type='function')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;margin-left:100px;text-align:right;background-color:lightgreen;color:black;'><b>LLM Tool Call</b><br><hr><br>get_thermostat_temperature({})</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:56:50 DEBUG:Executing tool: get_thermostat_temperature\n",
      "10:56:50 DEBUG:Calling no-arg function: get_thermostat_temperature\n",
      "10:56:50 DEBUG:get_thermostat_temperature called. Returning hardcoded 60\n",
      "10:56:50 DEBUG:✔️ function returned 60\n",
      "10:56:50 DEBUG:Executing tool_call: ChatCompletionMessageToolCall(id='call_u7GPkqIg03vpOWzBy97xtifd', function=Function(arguments='{\"temp\":70}', name='set_thermostat_temperature'), type='function')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;margin-left:100px;text-align:right;background-color:lightgreen;color:black;'><b>LLM Tool Call</b><br><hr><br>set_thermostat_temperature({\"temp\":70})</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:56:50 DEBUG:Executing tool: set_thermostat_temperature\n",
      "10:56:50 DEBUG:Attempting to deserialize {\"temp\":70} for tool: set_thermostat_temperature\n",
      "10:56:50 DEBUG:✔️ deserialized to temp=70.0. Calling function\n",
      "10:56:50 DEBUG:set_thermostat_temperature called with temp=70.0\n",
      "10:56:50 DEBUG:✔️ function returned \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;margin-right:100px;text-align:left;background-color:pink;color:black;'><b>Final LLM Response</b><br><hr><br>The temperature has been increased by 10 degrees to 70°F.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_chat_loop(\n",
    "    prompt=\"Increase the temperature by 10 degrees\",\n",
    "    tools = iot_tools\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
