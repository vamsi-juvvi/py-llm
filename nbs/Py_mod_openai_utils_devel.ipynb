{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb7fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow Colab or VSCore/Normal Jupyter environments\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# The default relative path when running the notebook from a cloned repo.\n",
    "LIB_PATH = Path(\"../lib\")\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print(\"üëâ Setting up for Colab\")\n",
    "    # This will create a py-llm dir at the same level as this notebook\n",
    "    # Refer to the lib in there using `./py-llm/lib` as opposed to the \n",
    "    # relative `../lib` when we are running straight from the py-llm/nbs \n",
    "    # directory in VScode.\n",
    "    if not os.path.isdir(\"./py-llm\"):\n",
    "        print(\"Cloning git repo into ./py-llm\")\n",
    "        !git clone -b 2_lib_use_openai_and_colab https://github.com/vamsi-juvvi/py-llm.git\n",
    "        LIB_PATH = Path(\"./py-llm/lib\")\n",
    "    else:\n",
    "        print(\"./py-llm exists. Not cloning\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "524cd7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append to sys.path directly\n",
    "# Make sure to `str(Path)`\n",
    "# - The resolve() converts relative to absolute. \n",
    "# - If you use ~ for HOME, use `Path.expand_user()`\n",
    "import os, sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(LIB_PATH.resolve()))\n",
    "\n",
    "from py_llm.util import jupyter_util\n",
    "from py_llm.util.jupyter_util import TextAlign\n",
    "from py_llm.util.jupyter_util import DisplayHTML as DH\n",
    "from py_llm.util.jupyter_util import DisplayMarkdown as DM\n",
    "from py_llm.util.jupyter_util import ColabEnv\n",
    "\n",
    "# Init logging at DEBUG. Once we are done testing, this can drop \n",
    "# down to warning\n",
    "jupyter_util.setup_logging(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7145a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for use in Colab or when the package is missing\n",
    "#!pip install -qy openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c5aff5",
   "metadata": {},
   "source": [
    "# Final API\n",
    "\n",
    "## module py_llm.llm.openai_util.py\n",
    "\n",
    "```\n",
    "Parent\n",
    " ‚îî‚îÄ‚îÄllm\n",
    "   ‚îú‚îÄ‚îÄ __init__.py\n",
    "   ‚îú‚îÄ‚îÄ openai_util.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c9df7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import logging\n",
    "\n",
    "openai.api_key = ColabEnv.colab_keyval_or_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "def get_completion(prompt, model=\"gpt-4o-mini\", temperature=0) -> str:\n",
    "    \"\"\"\n",
    "    Returns the one-shot completion of a simple prompt (no tools)\n",
    "    as a string response.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Completion Prompt : {prompt}\")\n",
    "    logging.debug(f\"model={model}, temp={temperature}\")\n",
    "\n",
    "    chat_history = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    response = get_response(chat_history, model, temperature)    \n",
    "    response_text = response.choices[0].message.content\n",
    "\n",
    "    # Could choose to log the entire response object too.    \n",
    "    logging.info(f\"Response Text: {response_text}\")\n",
    "    return response_text\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "def get_response(chat_history, model=\"gpt-4o-mini\", temperature=0):\n",
    "    \"\"\"\n",
    "    Returns the completion given a chat_history\n",
    "    \"\"\"    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=chat_history,\n",
    "        tools=None,\n",
    "        temperature=temperature)    \n",
    "    \n",
    "    return response\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "def chat_history_append_response(chat_history, response):\n",
    "    msg = response.choices[0].message\n",
    "    chat_history.append( {\n",
    "        \"role\" : msg.role,\n",
    "        \"content\" : msg.content\n",
    "    })\n",
    "\n",
    "    logging.debug(f\"Appended {chat_history[-1]} to chat_history\")\n",
    "\n",
    "    return chat_history\n",
    "\n",
    "def chat_history_append_user_msg(chat_history, content):    \n",
    "    chat_history.append( {\n",
    "        \"role\" : \"user\",\n",
    "        \"content\" : content\n",
    "    })\n",
    "\n",
    "    logging.debug(f\"Appended {chat_history[-1]} to chat_history\")\n",
    "\n",
    "    return chat_history    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e12757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:33:00 INFO:Completion Prompt : Why is the sky blue\n",
      "11:33:03 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "11:33:03 INFO:Response Text: The sky appears blue primarily due to a phenomenon called Rayleigh scattering. When sunlight enters the Earth's atmosphere, it is made up of different colors, each with varying wavelengths. Blue light has a shorter wavelength compared to other colors like red or yellow.\n",
      "\n",
      "As sunlight passes through the atmosphere, it collides with gas molecules and small particles. Because blue light is scattered in all directions more than other colors due to its shorter wavelength, we see a predominance of blue when we look up at the sky.\n",
      "\n",
      "During sunrise and sunset, the sun's light has to pass through a thicker layer of the atmosphere, which scatters the shorter blue wavelengths out of our line of sight and allows the longer wavelengths, like red and orange, to dominate, resulting in the beautiful colors we see at those times.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;text-align:justify;background-color:pink;color:black;'><b>OpenAI Response</b><br><hr><br>The sky appears blue primarily due to a phenomenon called Rayleigh scattering. When sunlight enters the Earth's atmosphere, it is made up of different colors, each with varying wavelengths. Blue light has a shorter wavelength compared to other colors like red or yellow.\n",
       "\n",
       "As sunlight passes through the atmosphere, it collides with gas molecules and small particles. Because blue light is scattered in all directions more than other colors due to its shorter wavelength, we see a predominance of blue when we look up at the sky.\n",
       "\n",
       "During sunrise and sunset, the sun's light has to pass through a thicker layer of the atmosphere, which scatters the shorter blue wavelengths out of our line of sight and allows the longer wavelengths, like red and orange, to dominate, resulting in the beautiful colors we see at those times.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ask why the sky is blue and print the response out in a color box\n",
    "completion = get_completion(\"Why is the sky blue\")\n",
    "DH.color_box(completion, \"OpenAI Response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec052eea",
   "metadata": {},
   "source": [
    "# Experiments and playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e85f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vamsi/mambaforge/envs/ml-pip/lib/python312.zip\n",
      "/home/vamsi/mambaforge/envs/ml-pip/lib/python3.12\n",
      "/home/vamsi/mambaforge/envs/ml-pip/lib/python3.12/lib-dynload\n",
      "\n",
      "/home/vamsi/mambaforge/envs/ml-pip/lib/python3.12/site-packages\n",
      "/home/vamsi/github/py-llm/lib\n"
     ]
    }
   ],
   "source": [
    "# Verify that the sys.path is as expected and contains out `py-llib/libs` path.\n",
    "print(\"\\n\".join(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a89f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;text-align:justify;background-color:pink;color:black;'>Hello in colorboox</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify that imports work!\n",
    "from py_llm.util.jupyter_util import DisplayHTML as DH\n",
    "DH.color_box(\"Hello in colorboox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cccbb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:33:16 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Examine the response object\n",
    "chat_history = [{\n",
    "    \"role\":\"user\", \n",
    "    \"content\":\"Why is the sky blue\"\n",
    "    }]\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=chat_history,\n",
    "        tools=None,\n",
    "        temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48006551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_pydantic_core_schema__',\n",
       " '__get_pydantic_json_schema__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__pydantic_complete__',\n",
       " '__pydantic_computed_fields__',\n",
       " '__pydantic_core_schema__',\n",
       " '__pydantic_custom_init__',\n",
       " '__pydantic_decorators__',\n",
       " '__pydantic_extra__',\n",
       " '__pydantic_fields__',\n",
       " '__pydantic_fields_set__',\n",
       " '__pydantic_generic_metadata__',\n",
       " '__pydantic_init_subclass__',\n",
       " '__pydantic_parent_namespace__',\n",
       " '__pydantic_post_init__',\n",
       " '__pydantic_private__',\n",
       " '__pydantic_root_model__',\n",
       " '__pydantic_serializer__',\n",
       " '__pydantic_validator__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__replace__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_recursion__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_check_frozen',\n",
       " '_copy_and_set_values',\n",
       " '_get_value',\n",
       " '_iter',\n",
       " 'annotations',\n",
       " 'audio',\n",
       " 'construct',\n",
       " 'content',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'from_orm',\n",
       " 'function_call',\n",
       " 'json',\n",
       " 'model_computed_fields',\n",
       " 'model_config',\n",
       " 'model_construct',\n",
       " 'model_copy',\n",
       " 'model_dump',\n",
       " 'model_dump_json',\n",
       " 'model_extra',\n",
       " 'model_fields',\n",
       " 'model_fields_set',\n",
       " 'model_json_schema',\n",
       " 'model_parametrized_name',\n",
       " 'model_post_init',\n",
       " 'model_rebuild',\n",
       " 'model_validate',\n",
       " 'model_validate_json',\n",
       " 'model_validate_strings',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'refusal',\n",
       " 'role',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'to_dict',\n",
       " 'to_json',\n",
       " 'tool_calls',\n",
       " 'update_forward_refs',\n",
       " 'validate']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There could be multiple choices depending on the parameters\n",
    "# top-k, max-p etc options. Just check the first one \n",
    "dir(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27038192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n"
     ]
    }
   ],
   "source": [
    "# Ok. I see\n",
    "#  role - Wanted to see if we need to manually insert the 'assistant` role when we \n",
    "#         add OpenAI response back to the chat_history or if we can simply take it \n",
    "#         from the response object itself.\n",
    "#  tool_calls (which we'll get to later)\n",
    "print(response.choices[0].message.role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45027e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"content\": \"The sky appears blue primarily due to a phenomenon called Rayleigh scattering. When sunlight enters the Earth's atmosphere, it is made up of different colors, each with varying wavelengths. Blue light has a shorter wavelength compared to other colors like red or yellow.\\n\\nAs sunlight passes through the atmosphere, it collides with gas molecules and small particles. Because blue light is scattered in all directions more than other colors due to its shorter wavelength, we see a predominance of blue when we look up at the sky.\\n\\nDuring sunrise and sunset, the sun's light has to pass through a thicker layer of the atmosphere, which scatters the shorter blue wavelengths out of our line of sight and allows the longer wavelengths, like red and orange, to dominate, giving the sky those warm colors.\",\n",
      "    \"refusal\": null,\n",
      "    \"role\": \"assistant\",\n",
      "    \"annotations\": [],\n",
      "    \"audio\": null,\n",
      "    \"function_call\": null,\n",
      "    \"tool_calls\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# How about the json field ? Whats that. The entire reponse in JSON format or something else ?\n",
    "# Of. The following shows that it is a method\n",
    "msg = response.choices[0].message\n",
    "type(msg.json)\n",
    "\n",
    "# msg.json() says deprecated and use model_dump_json() instead. This is a Pydantic model\n",
    "# Prints out stringified plain json in one line. To get plain json and use our own formatting use\n",
    "# msg.model_dump()\n",
    "#\n",
    "# The model name is so confusing here. It refers to the pydantic model (a data model), \n",
    "# not the LLM Model like \"gpt-4o-mini\"\n",
    "import json\n",
    "print(json.dumps(msg.model_dump(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bd0e9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:33:21 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "11:33:22 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "11:33:23 INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Try out multi-turn convo out.\n",
    "chat_history = [{\n",
    "    \"role\":\"user\", \n",
    "    \"content\":\"Tell me a clean joke about chefs\"\n",
    "    }]\n",
    "\n",
    "r = get_response(chat_history)\n",
    "chat_history_append_response(chat_history, r)\n",
    "chat_history_append_user_msg(chat_history, \"Summarize it\")\n",
    "r = get_response(chat_history)\n",
    "chat_history_append_response(chat_history, r)\n",
    "chat_history_append_user_msg(chat_history, \"Turn it into a Haiku\")\n",
    "r = get_response(chat_history)\n",
    "chat_history = chat_history_append_response(chat_history, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb4ce67",
   "metadata": {},
   "source": [
    "At this point, I decided that I can use color and HTML better and went back to [Py_mod_jupyter_utils_devel.ipynb](./Py_mod_jupyter_utils_devel.ipynb) for some more experiments. Ended up with \n",
    " - Specifiable `fg` and `bg` which default to \"black\" and \"pink\" respectively\n",
    " - Specifiable `text alignment` with a string enum: `TextAlign`\n",
    "   - Defaults to `TextAlign.DEFAULT` and `TextAlign.RIGHT` and `TextAlign.LEFT` are available\n",
    "   - Enhanced later with alignment affecting the margins so that the entire box matches the text alignment. When there is a lot of text completely filling up the box, the alignment's visual effect is lost unless the box is also shortened horizontally in the right way to display the alignment.\n",
    " - Node: The imports now have to include `TextAlign` as well. Since I changed the on-disk contents of the `jupyter_util.py` module, I need to restart the kernel here to import `TextAlign`   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "653092fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;margin-right:100px;text-align:left;background-color:orange;color:black;'>Tell me a clean joke about chefs</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;margin-left:100px;text-align:right;background-color:lightgreen;color:black;'>Why did the chef break up with their partner?\n",
       "\n",
       "Because they just couldn't find the thyme!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;margin-right:100px;text-align:left;background-color:orange;color:black;'>Summarize it</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;margin-left:100px;text-align:right;background-color:lightgreen;color:black;'>A chef broke up with their partner because they couldn't find the time.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;margin-right:100px;text-align:left;background-color:orange;color:black;'>Turn it into a Haiku</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='border-radius:15px;padding:15px;margin-left:100px;text-align:right;background-color:lightgreen;color:black;'>Chef's heart lost in time,  \n",
       "Love simmered, but couldn't cook‚Äî  \n",
       "Thyme slipped through their hands.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I find it more efficient to generate the responses separately\n",
    "# and break out the printing into another cell. That way, we are not \n",
    "# constantly making API calls to OpenAI etc. Get that done and then \n",
    "# experiment with how to print out the data. Otw have some way of \n",
    "# memoizing the get_response() function\n",
    "for h in chat_history:\n",
    "    role = h[\"role\"]\n",
    "    txt  = h[\"content\"]\n",
    "\n",
    "    match role:\n",
    "        case \"user\":\n",
    "            DH.color_box(txt, bg=\"orange\", align=TextAlign.LEFT)\n",
    "        case _:\n",
    "            assert(role == \"assistant\")\n",
    "            DH.color_box(txt, bg=\"lightgreen\", align=TextAlign.RIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54a31ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "[\n",
       "    {\n",
       "        \"role\": \"user\",\n",
       "        \"content\": \"Tell me a clean joke about chefs\"\n",
       "    },\n",
       "    {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Why did the chef break up with their partner?\\n\\nBecause they just couldn't find the thyme!\"\n",
       "    },\n",
       "    {\n",
       "        \"role\": \"user\",\n",
       "        \"content\": \"Summarize it\"\n",
       "    },\n",
       "    {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"A chef broke up with their partner because they couldn't find the time.\"\n",
       "    },\n",
       "    {\n",
       "        \"role\": \"user\",\n",
       "        \"content\": \"Turn it into a Haiku\"\n",
       "    },\n",
       "    {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Chef's heart lost in time,  \\nLove simmered, but couldn't cook\\u2014  \\nThyme slipped through their hands.\"\n",
       "    }\n",
       "]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# json.dumps(chat_history, indent=4) can be used \n",
    "# to dump this list. I can just print it out but \n",
    "# using Markdown with json syntax highlighting \n",
    "# will be nicer\n",
    "DM.md(DM.json_fmt(chat_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d936caa",
   "metadata": {},
   "source": [
    "# Colab Experiments\n",
    "\n",
    "The following was taken from google's docs. I then launched this notebook from colab using the https://colab.research.google.com/github/vamsi-juvvi/py-llm/blob/2_lib_use_openai_and_colab/nbs/Py_mod_openai_utils_devel.ipynb link and tested this cell. Once done, transfered this to the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625b9143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default relative path\n",
    "import os\n",
    "from pathlib import Path\n",
    "LIB_PATH = Path(\"../lib\")\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # This will create a py-llm dir at the same level as this notebook\n",
    "    # Refer to the lib in there using `./py-llm/lib` as opposed to the \n",
    "    # relative `../lib` when we are running straight from the py-llm/nbs \n",
    "    # directory in VScode.\n",
    "    if not os.path.isdir(\"./py-llm\"):\n",
    "        print(\"Cloning git repo into ./py-llm\")\n",
    "        !git clone -b 2_lib_use_openai_and_colab https://github.com/vamsi-juvvi/py-llm.git\n",
    "        LIB_PATH = Path(\"./py-llm/lib\")\n",
    "    else:\n",
    "        print(\"./py-llm exists. Not cloning\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac0b668",
   "metadata": {},
   "source": [
    "- ‚úîÔ∏è Make sure the above runs with no errors on colab\n",
    "  - ‚úîÔ∏è Clones correctly\n",
    "  - ‚úîÔ∏è The `LIB_PATH` is correct and allows us to import the `jpyter_utils` module.\n",
    "- ‚úîÔ∏è Setup the `OPENAI_API_KEY` key in secrets, enable it for this notebook and ensure that \n",
    "  - ‚úîÔ∏è `ColabEnv.colab_keyval(\"OPENAI_API_KEY\")` returns whatever was set\n",
    "  - ‚úîÔ∏è `ColabEnv.colab_keyval_or_env(\"OPENAI_API_KEY\")` works on colab and in VSCode\n",
    "- ‚úîÔ∏è Make the changes at the top of the notebook and then run all the cells from Colab.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
