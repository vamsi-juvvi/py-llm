{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReAct\n",
    "\n",
    "ReAct is one of the earliest _thinking_ prompting techniques where the LLM was asked to breakdown it's task into multiple turns and incorporate external tools in it's process before responding. The hope _(and finding)_ is that it would move away from simple recall to a more complex chain of thought yielding more appropriate answers.\n",
    "\n",
    "The most basic `ReAct` form is the one mentioned in the paper that introduced it : [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/pdf/2210.03629). See also the companion [react-lm.github.io](https://react-lm.github.io/) site.\n",
    "\n",
    "One of the confusing things about ReAct is that the prompts and examplars used vary widely. However, they all share a similar high-level structure: prompt engineering + COT exemplar to think along ReActy lines.\n",
    "\n",
    " - ReAct specific prompt\n",
    "   - Clarity of the ReAct instruction in the role\n",
    "   - Tool specification can vary: either fully natural language or mix in JSON notation for arguments\n",
    "   - Reduce hallucinations on failure: _Exemplar allowing the LLM to fail with a `I cannot answer this` instead of forcing it to hallucinate an answer_\n",
    " - General Prompt\n",
    "   - Re-iterate important instructions at the end\n",
    "   - Re-iterate function calling semantic rules to follow including JSON formats\n",
    "\n",
    "I find it useful to break the prompt down into sections so I have a common structure to study different examples. Otherwise, we end up with a wall-of-diffs that does not guide us toward tweaking our own prompts.\n",
    "\n",
    "## Experimentation results\n",
    "\n",
    "I started coding the ReAct loop based on the previously developed [tool infrastructure](./Py_mod_llm_agentic_toolcalling_devel.ipynb). While the conclusions were reached much further in this notebook, I will present their summary here for the impatient.\n",
    "\n",
    "### React activity\n",
    "\n",
    "![](../img/react_llm_next_steps.png)\n",
    "\n",
    "### ReAct overlayed on the chat protocol\n",
    "\n",
    "![](../img/react_chat_mm.png)\n",
    "\n",
    "----\n",
    "\n",
    "![](../img/react_highlevel_protocol.png)\n",
    "\n",
    "\n",
    "## Setup notebook py environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow Colab or VSCore/Normal Jupyter environments\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# The default relative path when running the notebook from a cloned repo.\n",
    "LIB_PATH = Path(\"../lib\")\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print(\"👉 Setting up for Colab\")\n",
    "    # This will create a py-llm dir at the same level as this notebook\n",
    "    # Refer to the lib in there using `./py-llm/lib` as opposed to the \n",
    "    # relative `../lib` when we are running straight from the py-llm/nbs \n",
    "    # directory in VScode.\n",
    "    if not os.path.isdir(\"./py-llm\"):\n",
    "        print(\"Cloning git repo into ./py-llm\")\n",
    "        !git clone https://github.com/vamsi-juvvi/py-llm.git\n",
    "        LIB_PATH = Path(\"./py-llm/lib\")\n",
    "    else:\n",
    "        print(\"./py-llm exists. Not cloning\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append to sys.path directly\n",
    "# Make sure to `str(Path)`\n",
    "# - The resolve() converts relative to absolute. \n",
    "# - If you use ~ for HOME, use `Path.expand_user()`\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "sys.path.append(str(LIB_PATH.resolve()))\n",
    "\n",
    "from py_llm.util import jupyter_util\n",
    "from py_llm.util.jupyter_util import TextAlign\n",
    "from py_llm.util.jupyter_util import DisplayHTML as DH\n",
    "from py_llm.util.jupyter_util import DisplayMarkdown as DM\n",
    "from py_llm.util.jupyter_util import ColabEnv\n",
    "\n",
    "from py_llm.llm import openai_util as oai\n",
    "from py_llm.llm.tools import Tool, ToolCollection\n",
    "\n",
    "# Init jupyter env\n",
    "jupyter_util.setup_logging(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for use in Colab or when the package is missing\n",
    "#!pip install -qy openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# If you want to log OpenAI's python library itself, also set the log level for this\n",
    "# normally, limit this to warning/error and keep your own logging at debug levels.\n",
    "# If this doesn't work right away, restart the kernel after changing the log-level\n",
    "os.environ[\"OPENAI_LOG\"]=\"error\"\n",
    "\n",
    "# Finally ensure you have the OpenAI key.\n",
    "openai.api_key = ColabEnv.colab_keyval_or_env(\"OPENAI_API_KEY\")\n",
    "assert(openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Build on top of existing tool-calling agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import py_llm.llm.agentic.tool_calling as tooled_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ReAct system prompt structure and template\n",
    "\n",
    "The examples you see online are huge blocks of text. There is a structure to the pompt but it is obscured by all the text. I want to make the structure explicit so it is easier to reason about it and maybe tune it in sections.\n",
    "\n",
    "I will use the following template as the initial breakdown (based on prompt at https://docs.llamaindex.ai/en/stable/examples/agent/react_agent/) and tune it as I go along. The goal is to end up with a template and a `builder` to craft the ReAct system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "\n",
    "react_system_prompt_template = Template(\"\"\"\n",
    "                                        \n",
    "$YOUR_ROLE_AS_REACT_SECTION\n",
    "                                        \n",
    "$REACT_TOOLS_SECTION\n",
    "\n",
    "$REACT_LOOP_EXEMPLARS_SECTION\n",
    "                                        \n",
    "$REACT_LOOP_ADDITIONAL_RULES\n",
    "                                        \n",
    "$REACT_INTRODUCE_CONVERSATION_SECTION\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the following dictionary to hold the substitution variables _(Template terminology: a dictionary keyed by the variable names will will eventually replace the variables in the template)_. Some of the values are themselves _Templates_ if their semantics allows for further deconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "react_sys_prompt_template_args = {}\n",
    "\n",
    "react_sys_prompt_template_args[\"YOUR_ROLE_AS_REACT_SECTION\"] = \"\"\"\n",
    "You are designed to help with a variety of tasks, from answering questions \\\n",
    "to providing summaries to other types of analyses.\"\"\".strip()\n",
    "\n",
    "# + TOOLS\n",
    "react_sys_prompt_template_args[\"REACT_TOOLS_SECTION\"] = Template(\"\"\"\n",
    "## Tools\n",
    "You have access to a wide variety of tools. You are responsible for using\n",
    "the tools in any sequence you deem appropriate to complete the task at hand.\n",
    "This may require breaking the task into subtasks and using different tools\n",
    "to complete each subtask.\n",
    "\n",
    "You have access to the following tools:\n",
    "$TOOLS\n",
    "\"\"\".strip())\n",
    "\n",
    "# + TOOL_NAMES_CSV\n",
    "# + REACT_CONCLUSION_WITH_SUCCESS_EXEMPLAR\n",
    "#     Thought: I can answer without using any more tools.\n",
    "#     Answer: [your answer here]\n",
    "#\n",
    "# + REACT_CONCLUSION_WITH_FAILURE_EXEMPLAR\n",
    "#     Thought: I cannot answer the question with the provided tools.\n",
    "#     Answer: Sorry, I cannot answer your query.\n",
    "react_sys_prompt_template_args[\"REACT_LOOP_EXEMPLARS_SECTION\"] = Template(\"\"\"\n",
    "## Output Format\n",
    "To answer the question, please use the following format.\n",
    "\n",
    "```\n",
    "Thought: I need to use a tool to help me answer the question.\n",
    "Action: tool name (one of $TOOL_NAMES_CSV) if using a tool.\n",
    "Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n",
    "```\n",
    "\n",
    "Please ALWAYS start with a Thought.\n",
    "\n",
    "Please use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.\n",
    "\n",
    "If this format is used, the user will respond with an observation in the following format:\n",
    "\n",
    "```\n",
    "Observation: tool response\n",
    "```\n",
    "\n",
    "You should keep repeating the above format until you have enough information\n",
    "to answer the question without using any more tools. At that point, you MUST respond\n",
    "in the one of the following two formats:\n",
    "\n",
    "```\n",
    "$REACT_CONCLUSION_WITH_SUCCESS_EXEMPLAR\n",
    "```\n",
    "\n",
    "```\n",
    "$REACT_CONCLUSION_WITH_FAILURE_EXEMPLAR\n",
    "```\n",
    "                                                                          \n",
    "Please Pay attention to the following instructions:\n",
    "  - You MUST obey the function signature of each tool. Do NOT pass in no arguments if the function expects arguments.\n",
    "\"\"\".strip())\n",
    "\n",
    "# In the LlamaIndex example, this was done as an enhancement step when \n",
    "# refining the prompt. Retain it as the LLAMA_REACT_LOOP_ADDITIONAL_RULES \n",
    "# constant (if you want to try it out) but default it to \"\".\n",
    "react_sys_prompt_template_args[\"REACT_LOOP_ADDITIONAL_RULES\"] = \"\"\n",
    "\n",
    "react_sys_prompt_template_args[\"REACT_INTRODUCE_CONVERSATION_SECTION\"] = \"\"\"\n",
    "## Current Conversation\n",
    "Below is the current conversation consisting of interleaving human and assistant messages.\n",
    "\"\"\".strip()\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "# Extra constants\n",
    "#--------------------------------------------------------------------------\n",
    "# Possible values to try for additional rules. Taken from the LlamaIndex \n",
    "# examples\n",
    "LLAMA_REACT_LOOP_ADDITIONAL_RULES = \"\"\"\n",
    "## Additional Rules\n",
    "- The answer MUST contain a sequence of bullet points that explain how you arrived at the answer. This can include aspects of the previous conversation history.        \n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def are_all_vars_resolved(tmpl: Template) -> bool :\n",
    "        try:\n",
    "            tmpl.substitute({})\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False        \n",
    "\n",
    "class ReactSysPromptBuilder:    \n",
    "\n",
    "    def __init__(self):        \n",
    "        # the keys of subst_args will be resolved incrementally. So use a \n",
    "        # deep copy to leave template args intact.\n",
    "        self.subst_args = copy.deepcopy(react_sys_prompt_template_args)\n",
    "        self.tmpl       = react_system_prompt_template\n",
    "\n",
    "    #-------------------------------------    \n",
    "    def build_safe(self) -> str:\n",
    "         \"\"\"Does not fail even if variables are unresolved\"\"\"\n",
    "         # low level resolve in a copy\n",
    "         resolved_args = copy.deepcopy(self.subst_args)\n",
    "         for k,v in resolved_args.items():\n",
    "              if isinstance(v, Template):\n",
    "                   resolved_args[k] = v.safe_substitute({})\n",
    "\n",
    "         return self.tmpl.safe_substitute(resolved_args)\n",
    "\n",
    "    #-------------------------------------\n",
    "    def override_role(self, role_arg: str | None):\n",
    "         \"\"\"\n",
    "         Call if you want a different role than the default\n",
    "         ReAct role.\n",
    "         \"\"\"\n",
    "         ROLE_SECTION_KEY = \"YOUR_ROLE_AS_REACT_SECTION\"\n",
    "         if role_arg:\n",
    "            self.subst_args[ROLE_SECTION_KEY] = role_arg        \n",
    "\n",
    "    #-------------------------------------\n",
    "    def init_tools_tmpl(self, tools_arg:str):\n",
    "        TOOLS_SECTION_KEY=\"REACT_TOOLS_SECTION\"\n",
    "        TOOLS_CHILD_KEY=\"TOOLS\"          \n",
    "\n",
    "        # Update the EXEMPLARSSECTION_KEY template\n",
    "        self._do_update_tmpl_arg(\n",
    "             TOOLS_SECTION_KEY,\n",
    "             {\n",
    "                TOOLS_CHILD_KEY : tools_arg if tools_arg else \"TOOLS is NOT SPECIFIED\"\n",
    "             })            \n",
    "    \n",
    "    #-------------------------------------\n",
    "    def init_exemplars_tmpl(self, \n",
    "                            tool_names_csv : str,\n",
    "                            success_example:str | None, \n",
    "                            cannot_answer_example: str | None):\n",
    "        \"\"\"\n",
    "        Must be called to set the tool_names_csv as it has no meaningful default.\n",
    "        \n",
    "        The success_example and cannot_answer_examples can be overridden \n",
    "        but have default values.\n",
    "        \"\"\"\n",
    "\n",
    "        EXEMPLARSSECTION_KEY=\"REACT_LOOP_EXEMPLARS_SECTION\"\n",
    "\n",
    "        CHILD_KEY_TOOL_NAMES_CSV = \"TOOL_NAMES_CSV\"\n",
    "        CHILD_KEY_SUCCESS        = \"REACT_CONCLUSION_WITH_SUCCESS_EXEMPLAR\"\n",
    "        CHILD_KEY_CANNOT_ANSWER  = \"REACT_CONCLUSION_WITH_FAILURE_EXEMPLAR\"\n",
    "\n",
    "        # Defaults        \n",
    "        DEFAULT_SUCCESS_EXAMPLE = \"\"\"\n",
    "Thought: I can answer without using any more tools.\n",
    "Answer: [your answer here]\n",
    "            \"\"\".strip()\n",
    "             \n",
    "        DEFAULT_CANNOT_ANSWER_EXAMPLE = \"\"\"\n",
    "Thought: I cannot answer the question with the provided tools.\n",
    "Answer: Sorry, I cannot answer your query.\n",
    "             \"\"\".strip()        \n",
    "\n",
    "        # Update the EXEMPLARSSECTION_KEY template\n",
    "        self._do_update_tmpl_arg(\n",
    "             EXEMPLARSSECTION_KEY,\n",
    "             {\n",
    "                CHILD_KEY_TOOL_NAMES_CSV : tool_names_csv if tool_names_csv else \"Tool names NOT SPECIFIED\",\n",
    "                CHILD_KEY_SUCCESS        : success_example if success_example else DEFAULT_SUCCESS_EXAMPLE,\n",
    "                CHILD_KEY_CANNOT_ANSWER  : cannot_answer_example if cannot_answer_example else DEFAULT_CANNOT_ANSWER_EXAMPLE,\n",
    "            })                \n",
    "        \n",
    "    #-------------------------------------\n",
    "    def init_additional_rules_tmpl(self, \n",
    "                            additional_rules : str | None = None):\n",
    "        \"\"\"\n",
    "        You don't always want additional rules. \n",
    "        Defaults to empty \"\". \n",
    "\n",
    "        Call with REACT_LOOP_ADDITIONAL_RULES constant if you want to try \n",
    "        the 'conclude with reasons` version that LlamaIndex tried in their refined\n",
    "        version of the prompt.\n",
    "        \"\"\"        \n",
    "        ADDITIONAL_RULES_SECTION_KEY=\"REACT_LOOP_ADDITIONAL_RULES\"                \n",
    "        self._do_update_string_arg(\n",
    "            ADDITIONAL_RULES_SECTION_KEY,\n",
    "            additional_rules\n",
    "        )         \n",
    "    \n",
    "    #-------------------------------------\n",
    "    def _do_update_tmpl_arg(self, key:str, subst_dict:dict):\n",
    "        \"\"\"\n",
    "        Updates self.subst_args[key] 's template value with the supplied\n",
    "        substitutions. The updated value will remain a Template when all\n",
    "        values are fully resolved.\n",
    "        \n",
    "        This is done safely with no exceptions which means \n",
    "        the resulting template can still have unresolved variables. \n",
    "        \"\"\"\n",
    "        assert(isinstance( self.subst_args[key], Template))\n",
    "\n",
    "        self.subst_args[key] = Template(\n",
    "                 self.subst_args[key].safe_substitute(subst_dict)\n",
    "        )\n",
    "    \n",
    "    def _do_update_string_arg(self, key:str, subst_str:str|None=None):\n",
    "        \"\"\"\n",
    "        Replaces self.subst_args[key] 's string value with the supplied\n",
    "        string. \n",
    "\n",
    "        If None is suppplied, then this is cleared out (set to \"\")\n",
    "        \"\"\"\n",
    "        assert(isinstance( self.subst_args[key], str))\n",
    "\n",
    "        self.subst_args[key] = subst_str if subst_str else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "# ReAct - LlamaIndex example\n",
    "\n",
    "See https://docs.llamaindex.ai/en/stable/examples/agent/react_agent/\n",
    "\n",
    "Recreate this from the above system prompt. This will be straightforward since the template was deconstructed from this prompt.\n",
    "\n",
    "Note:\n",
    " - They keep the `REACT_LOOP_ADDITIONAL_RULES` section blank first time around\n",
    " - Setting it to `LLAMA_REACT_LOOP_ADDITIONAL_RULES` apparently leads to it describing the actions it took.\n",
    "\n",
    "**Caveat**: For some models that support tool-calling natively, they say that you might need an option to map `Observation` to `role=\"tool\"` used in typical tool-call responses _(for OpenAI atleast)_. They that this is handled via \n",
    "\n",
    "```python\n",
    "from llama_index.core.agent import ReActChatFormatter\n",
    "from llama_index.core.llms import MessageRole\n",
    "\n",
    "agent = ReActAgent.from_tools(\n",
    "    [multiply_tool, add_tool],\n",
    "    llm=llm,\n",
    "    react_chat_formatter=ReActChatFormatter.from_defaults(\n",
    "        observation_role=MessageRole.TOOL\n",
    "    ),\n",
    "    verbose=True,\n",
    ")\n",
    "```\n",
    "\n",
    "This should be straigtforward to do for me as well as specifying a `\"role\" : blah` is standard procedure.\n",
    "\n",
    "When exercisig the LlamaIndex simple use-case, I had to enhance my tool framework (See [Py_mod_llm_tools_devel.ipynb](./Py_mod_llm_tools_devel.ipynb) and [Py_mod_llm_toolcalling_devel.ipynb](./Py_mod_llm_toolcalling_devel.ipynb))\n",
    " - ✔️ Already allows tools of the form `fn(PydanticModel)`\n",
    " - ➕ Now allows regular python methods of the form `fn(a, b, c)`. Tested for primitive types and will fix limitations as I come across them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.llamaindex.ai/en/stable/examples/agent/react_agent/\n",
    "# uses the following tools.\n",
    "def multiply(a:int, b:int) -> int:\n",
    "    \"\"\"Multiply two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def add(a:int, b:int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# Add these to a tool-collection\n",
    "react_1_tc = ToolCollection([multiply, add])\n",
    "\n",
    "# sanity checks\n",
    "if 0:\n",
    "    # test out the schemas needed for ReAct: get_inprompt_schemas()\n",
    "    # Note that for regular tool calling, we use the get_schemas() method.\n",
    "    for ts in react_1_tc.get_inprompt_schemas(mapper = lambda ps: str(ps)):\n",
    "        DM.hr()\n",
    "        DM.code(ts)\n",
    "\n",
    "    print(react_1_tc.get_tool_names())\n",
    "    DM.hr()\n",
    "\n",
    "    # test direct execution\n",
    "    import json\n",
    "    print(\n",
    "        react_1_tc.exec_tool(\"add\", json.dumps({\"a\":3, \"b\":8}))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are designed to help with a variety of tasks, from answering questions to providing summaries to other types of analyses.\n",
      "\n",
      "## Tools\n",
      "You have access to a wide variety of tools. You are responsible for using\n",
      "the tools in any sequence you deem appropriate to complete the task at hand.\n",
      "This may require breaking the task into subtasks and using different tools\n",
      "to complete each subtask.\n",
      "\n",
      "You have access to the following tools:\n",
      "\n",
      "Tool Name : multiply\n",
      "Tool Description : Multiply two integers and returns the result integer\n",
      "Tool Args: {\"properties\": {\"a\": {\"title\": \"A\", \"type\": \"integer\"}, \"b\": {\"title\": \"B\", \"type\": \"integer\"}}, \"required\": [\"a\", \"b\"], \"title\": \"multiply_args\", \"type\": \"object\", \"additionalProperties\": false}\n",
      "\n",
      "Tool Name : add\n",
      "Tool Description : Add two integers and returns the result integer\n",
      "Tool Args: {\"properties\": {\"a\": {\"title\": \"A\", \"type\": \"integer\"}, \"b\": {\"title\": \"B\", \"type\": \"integer\"}}, \"required\": [\"a\", \"b\"], \"title\": \"add_args\", \"type\": \"object\", \"additionalProperties\": false}\n",
      "\n",
      "## Output Format\n",
      "To answer the question, please use the following format.\n",
      "\n",
      "```\n",
      "Thought: I need to use a tool to help me answer the question.\n",
      "Action: tool name (one of multiply, add) if using a tool.\n",
      "Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n",
      "```\n",
      "\n",
      "Please ALWAYS start with a Thought.\n",
      "\n",
      "Please use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.\n",
      "\n",
      "If this format is used, the user will respond with an observation in the following format:\n",
      "\n",
      "```\n",
      "Observation: tool response\n",
      "```\n",
      "\n",
      "You should keep repeating the above format until you have enough information\n",
      "to answer the question without using any more tools. At that point, you MUST respond\n",
      "in the one of the following two formats:\n",
      "\n",
      "```\n",
      "Thought: I can answer without using any more tools.\n",
      "Answer: [your answer here]\n",
      "```\n",
      "\n",
      "```\n",
      "Thought: I cannot answer the question with the provided tools.\n",
      "Answer: Sorry, I cannot answer your query.\n",
      "```\n",
      "\n",
      "Please Pay attention to the following instructions:\n",
      "  - You MUST obey the function signature of each tool. Do NOT pass in no arguments if the function expects arguments.\n",
      "\n",
      "\n",
      "\n",
      "## Current Conversation\n",
      "Below is the current conversation consisting of interleaving human and assistant messages.\n"
     ]
    }
   ],
   "source": [
    "builder = ReactSysPromptBuilder()\n",
    "\n",
    "builder.init_tools_tmpl(\n",
    "    tools_arg = \"\\n\" + \"\\n\\n\".join(\n",
    "        react_1_tc.get_inprompt_schemas(mapper = lambda ps: str(ps))\n",
    "    )\n",
    ")\n",
    "\n",
    "builder.init_exemplars_tmpl(\n",
    "    tool_names_csv = \", \".join(react_1_tc.get_tool_names()),\n",
    "    success_example = None,\n",
    "    cannot_answer_example = None)\n",
    "\n",
    "builder.init_additional_rules_tmpl(\n",
    "    additional_rules=None)\n",
    "\n",
    "print(builder.build_safe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "\n",
    "class ReactAssistantResponse:\n",
    "    PATTERN_TH        = re.compile(r\"^(Thought|Action|Action Input|Answer)\\s*:\\s*(.*?)$\", re.MULTILINE)\n",
    "    PATTERN_FUNC_NAME = re.compile(r\"^\\s*(?:function[s]?\\.)?(.*)$\")\n",
    "\n",
    "    def __init__(self, assistant_response:str):\n",
    "        self.thought      = None\n",
    "        self.action       = None\n",
    "        self.action_input = None\n",
    "        self.answer       = None\n",
    "\n",
    "        #-- Parse -----------------        \n",
    "        match_list = self.PATTERN_TH.findall(assistant_response)\n",
    "\n",
    "        if match_list or len(match_list) > 0:\n",
    "            d = {}\n",
    "            for m in match_list:\n",
    "                key = m[0]\n",
    "                val = m[1]\n",
    "                logging.debug(f\"Extracted [{key} = {val}] pair\")\n",
    "                d[key] = val\n",
    "            self._init_kvps(d)\n",
    "        else:\n",
    "            logging.debug(\"Could not extract any exected React semantic sections from assistant response\\n{assistant_response}\")\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"\"\"\n",
    "Thought     : {self.thought}\n",
    "Action      : {self.action}\n",
    "Action Input: {self.action_input}\"\"\".strip()\n",
    "\n",
    "    def _init_kvps(self, d):\n",
    "        for k,v in d.items():\n",
    "            match k.lower():\n",
    "                case \"thought\":\n",
    "                    self.thought = v.strip()\n",
    "                case \"action\":\n",
    "                    # These seem to sometimes comes in as `function.my_action`\n",
    "                    fn_match = self.PATTERN_FUNC_NAME.match(v)\n",
    "                    if fn_match:\n",
    "                        self.action = fn_match.group(1)\n",
    "                        logging.debug(f\"Got Action = {self.action}\")\n",
    "                    else:\n",
    "                        logging.error(f\"Unable to get function name from Action=\\\"{v}\\\"\")\n",
    "                case \"action input\":\n",
    "                    self.action_input = v\n",
    "                    logging.debug(f\"Got Action Input = {v}\")\n",
    "                case \"answer\":\n",
    "                    self.answer = v\n",
    "                    logging.debug(f\"Got Answer = {v}\")\n",
    "                case _:\n",
    "                    logging.warning(f\"Unknown Key={k} with Value={v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReactObservation:    \n",
    "    def format_observation(content:str):\n",
    "        # Note that our system prompts tells the LLM to expect the Observation\n",
    "        # in triple single-quotes.\n",
    "        return f\"\"\"```\n",
    "Observation: {content}\n",
    "```\n",
    "\"\"\"\n",
    "    \n",
    "    def from_action_response(tool_response: str):\n",
    "        return ReactObservation.format_observation(\n",
    "            tool_response\n",
    "        )\n",
    "    \n",
    "    def from_action_error(ar:ReactAssistantResponse, e):\n",
    "        return ReactObservation.format_observation(\n",
    "            f\"There was an error executing `Action: {ar.action}` with `Action Input: {ar.action_input}. \"\n",
    "            f\"The python excepion is as follows: {str(e)}. \"\n",
    "            f\"If possible, try again with a different action or different inputs. Remember, pay attention to JSON formatting\"\n",
    "        )\n",
    "\n",
    "    def from_missing_action(assistant_response:ReactAssistantResponse, tools: ToolCollection):\n",
    "        return ReactObservation.format_observation(\n",
    "            f\"The Action `{assistant_response.action}` is unknown. Cannot execute it. \"\n",
    "            f\"Try one of the available ones [{\", \".join(tools.get_tool_names())}]\"\n",
    "        )\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def react_observation_from_action(ar:ReactAssistantResponse, tools : ToolCollection):\n",
    "    \"\"\"\n",
    "    The Observation indicates continutation. If this function returns None, that means\n",
    "    the react-lop has ended.\n",
    "    \"\"\"\n",
    "    if ar.answer:\n",
    "        logging.debug(\"Terminating ReAct loop as an answer has been provided.\")\n",
    "    else:\n",
    "        logging.debug(\"Continuing react loop. Executing Action asked for.\")\n",
    "        assert(ar.action)\n",
    "\n",
    "        if tools.has_tool(ar.action):\n",
    "            try:\n",
    "                logging.debug(f\"Executing action/tool {ar.action} with args {ar.action_input}\")\n",
    "                action_response = tools.exec_tool(\n",
    "                    name = ar.action, \n",
    "                    args = ar.action_input)\n",
    "                \n",
    "                return ReactObservation.from_action_response(str(action_response))\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Executing action raise {str(e)}\")\n",
    "                return ReactObservation.from_action_error(ar, e)\n",
    "        else:\n",
    "            logging.error(f\"Assistant asked for Action:{ar.action}. There is no such tool!\")\n",
    "            return ReactObservation.from_missing_action(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start with the tool_calling agent's run_chat_loop method.\n",
    "def run_react_loop(sys_prompt: str, start_prompt:str, tools : ToolCollection | None):\n",
    "    \"\"\"\n",
    "    Runs a chat loop with an initial prompt and supplied tools\n",
    "    Resolves all tool_calls made till a final assistant response is provided\n",
    "\n",
    "    If a tool_call is made by the LLM and no tools are supplied, a ValueError is raised.\n",
    "    \"\"\"\n",
    "    if not sys_prompt  : raise ValueError(\"run_react_loop: `sys_prompt` must be supplied\")\n",
    "    if not start_prompt: raise ValueError(\"run_react_loop: `start_prompt` must be supplied\")\n",
    "\n",
    "    # Initialize\n",
    "    chat_history = [ { \"role\" : \"system\", \"content\" : sys_prompt}]    \n",
    "\n",
    "    # Not sure if we should be supplying tools via `tools=` or only \n",
    "    # executing the ones that are embedded in the sys_prompt ?\n",
    "    # For now, null this out.\n",
    "    tool_schemas = [] # tools.get_schemas() if tools else []\n",
    "\n",
    "    # Run the loop\n",
    "    # The msgs list also controls loop continutation. When msgs is empty, \n",
    "    # the loop ends\n",
    "    msgs = [{\n",
    "        \"role\":\"user\", \n",
    "        \"content\": start_prompt}]\n",
    "    \n",
    "    while len(msgs):\n",
    "        chat_history.extend(msgs)\n",
    "        msgs = []\n",
    "\n",
    "        response = oai.get_response(\n",
    "            chat_history=chat_history,\n",
    "            tools = tool_schemas)\n",
    "\n",
    "        # tool-call\n",
    "        # Note: The OpenAI example is outdated\n",
    "        # tool_calls is not longer a JSON object but an array of \n",
    "        # `ChatCompletionMessageToolCall` objects\n",
    "        if response.choices[0].message.tool_calls:\n",
    "\n",
    "            # We do not expect actual tool_calls\n",
    "            # These comes in indirectly via an assistant response that \n",
    "            # asks for an 'Action`.\n",
    "            # We could later extend this into either\n",
    "            #   - An observation that it is making a tool-call instead of \n",
    "            #     sending an action. Turn this exception into an Observation.\n",
    "            raise NotImplementedError(\"Got tool-call in react-loop. Not implemented\")            \n",
    "        else:\n",
    "            # Assistant response\n",
    "            chat_response = response.choices[0].message.content       \n",
    "            DH.text(\n",
    "                \"<br>\".join(f\"Assistant: {chat_response}\".split('\\n')),\n",
    "                fg=\"green\")            \n",
    "            logging.info(f\"Received assistant response : {chat_response}\")\n",
    "\n",
    "            # Add response to chat response\n",
    "            chat_history.append({\n",
    "                \"role\" : \"assistant\",\n",
    "                \"content\" : chat_response\n",
    "            })\n",
    "\n",
    "            # Followup on the assistant response\n",
    "            parsed_response = ReactAssistantResponse(chat_response)\n",
    "            print(str(parsed_response))\n",
    "\n",
    "            match react_observation_from_action(parsed_response, tools):\n",
    "                case None:\n",
    "                    # Nothing added to msgs.\n",
    "                    # loop will end.\n",
    "                    logging.debug(\"🛑 React loop is terminated\")\n",
    "                case o:                    \n",
    "                    DH.text(o, fg=\"black\", bg=\"orange\")\n",
    "                    msgs.append({\n",
    "                        \"role\" : \"user\",\n",
    "                        \"content\": o\n",
    "                    })\n",
    "\n",
    "    \n",
    "    # return final chat_history item as the response.\n",
    "    # assert that it is from assistant ?\n",
    "    return chat_history[-1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=';color:green;;'>Assistant: ```<br>Thought: I need to calculate the expression step by step, starting with the multiplication.<br>Action: functions.multiply<br>Action Input: {\"a\": 2, \"b\": 4}<br>```</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought     : I need to calculate the expression step by step, starting with the multiplication.\n",
      "Action      : multiply\n",
      "Action Input: {\"a\": 2, \"b\": 4}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style='background-color:orange;color:black;;'>```\n",
       "Observation: 8\n",
       "```\n",
       "</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p style=';color:green;;'>Assistant: ```<br>Thought: Now that I have the result of the multiplication (2 * 4 = 8), I can proceed to add it to 20.<br>Action: functions.add<br>Action Input: {\"a\": 20, \"b\": 8}<br>```</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought     : Now that I have the result of the multiplication (2 * 4 = 8), I can proceed to add it to 20.\n",
      "Action      : add\n",
      "Action Input: {\"a\": 20, \"b\": 8}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style='background-color:orange;color:black;;'>```\n",
       "Observation: 28\n",
       "```\n",
       "</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p style=';color:green;;'>Assistant: ```<br>Thought: I can answer without using any more tools.<br>Answer: The result of the expression 20 + (2 * 4) is 28.<br>```</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought     : I can answer without using any more tools.\n",
      "Action      : None\n",
      "Action Input: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'```\\nThought: I can answer without using any more tools.\\nAnswer: The result of the expression 20 + (2 * 4) is 28.\\n```'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is 20+(2*4)? Calculate step by step\n",
    "sys_prompt     = builder.build_safe()\n",
    "react_question = \"What is 20+(2*4)? Calculate step by step\"\n",
    "\n",
    "run_react_loop(sys_prompt=sys_prompt, \n",
    "               start_prompt=react_question, \n",
    "               tools=react_1_tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad\n",
    "\n",
    "Snippets while building the loops\n",
    "\n",
    "## Develop the react loop\n",
    "\n",
    " - Start with the tool_calling agent's run_chat_loop method.\n",
    " - Just send prompt through and see if&how the assistant response comes through\n",
    " - Create new `ReactResponse` class to parse the assistant response \n",
    " - Incorporate the `ReactResponse` into the react loop\n",
    "   - new `react_observation_from_action`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 👉 Run the Cell above to call run_react_loop() once you execute this cell\n",
    "# that will make it pickup the ReactObservation and run_react_loop from this cell\n",
    "import py_llm.llm.openai_util as oai\n",
    "from   py_llm.llm.tools import ToolCollection\n",
    "\n",
    "class ReactObservation:    \n",
    "    def format_observation(content:str):\n",
    "        # Note that our system prompts tells the LLM to expect the Observation\n",
    "        # in triple single-quotes.\n",
    "        return f\"\"\"```\n",
    "Observation: {content}\n",
    "```\n",
    "\"\"\"\n",
    "    \n",
    "    def from_action_response(tool_response: str):\n",
    "        return ReactObservation.format_observation(\n",
    "            tool_response\n",
    "        )\n",
    "    \n",
    "    def from_action_error(ar:ReactAssistantResponse, e):\n",
    "        return ReactObservation.format_observation(\n",
    "            f\"There was an error executing `Action: {ar.action}` with `Action Input: {ar.action_input}. \"\n",
    "            f\"The python excepion is as follows: {str(e)}. \"\n",
    "            f\"If possible, try again with a different action or different inputs. Remember, pay attention to JSON formatting\"\n",
    "        )\n",
    "\n",
    "    def from_missing_action(assistant_response:ReactAssistantResponse, tools: ToolCollection):\n",
    "        return ReactObservation.format_observation(\n",
    "            f\"The Action `{assistant_response.action}` is unknown. Cannot execute it. \"\n",
    "            f\"Try one of the available ones [{\", \".join(tools.get_tool_names())}]\"\n",
    "        )\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def react_observation_from_action(ar:ReactAssistantResponse, tools : ToolCollection):\n",
    "    \"\"\"\n",
    "    The Observation indicates continutation. If this function returns None, that means\n",
    "    the react-lop has ended.\n",
    "    \"\"\"\n",
    "    if ar.answer:\n",
    "        logging.debug(\"Terminating ReAct loop as an answer has been provided.\")\n",
    "    else:\n",
    "        logging.debug(\"Continuing react loop. Executing Action asked for.\")\n",
    "        assert(ar.action)\n",
    "\n",
    "        if tools.has_tool(ar.action):\n",
    "            try:\n",
    "                logging.debug(f\"Executing action/tool {ar.action} with args {ar.action_input}\")\n",
    "                action_response = tools.exec_tool(\n",
    "                    name = ar.action, \n",
    "                    args = ar.action_input)\n",
    "                \n",
    "                return ReactObservation.from_action_response(str(action_response))\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Executing action raise {str(e)}\")\n",
    "                return ReactObservation.from_action_error(ar, e)\n",
    "        else:\n",
    "            logging.error(f\"Assistant asked for Action:{ar.action}. There is no such tool!\")\n",
    "            return ReactObservation.from_missing_action(ar)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def run_react_loop(sys_prompt: str, start_prompt:str, tools : ToolCollection | None):\n",
    "    \"\"\"\n",
    "    Runs a chat loop with an initial prompt and supplied tools\n",
    "    Resolves all tool_calls made till a final assistant response is provided\n",
    "\n",
    "    If a tool_call is made by the LLM and no tools are supplied, a ValueError is raised.\n",
    "    \"\"\"\n",
    "    if not sys_prompt  : raise ValueError(\"run_react_loop: `sys_prompt` must be supplied\")\n",
    "    if not start_prompt: raise ValueError(\"run_react_loop: `start_prompt` must be supplied\")\n",
    "\n",
    "    # Initialize\n",
    "    chat_history = [ { \"role\" : \"system\", \"content\" : sys_prompt}]    \n",
    "\n",
    "    # Not sure if we should be supplying tools via `tools=` or only \n",
    "    # executing the ones that are embedded in the sys_prompt ?\n",
    "    # For now, null this out.\n",
    "    tool_schemas = [] # tools.get_schemas() if tools else []\n",
    "\n",
    "    # Run the loop\n",
    "    # The msgs list also controls loop continutation. When msgs is empty, \n",
    "    # the loop ends\n",
    "    msgs = [{\n",
    "        \"role\":\"user\", \n",
    "        \"content\": start_prompt}]\n",
    "    \n",
    "    while len(msgs):\n",
    "        chat_history.extend(msgs)\n",
    "        msgs = []\n",
    "\n",
    "        response = oai.get_response(\n",
    "            chat_history=chat_history,\n",
    "            tools = tool_schemas)\n",
    "\n",
    "        # tool-call\n",
    "        # Note: The OpenAI example is outdated\n",
    "        # tool_calls is not longer a JSON object but an array of \n",
    "        # `ChatCompletionMessageToolCall` objects\n",
    "        if response.choices[0].message.tool_calls:\n",
    "\n",
    "            # We do not expect actual tool_calls\n",
    "            # These comes in indirectly via an assistant response that \n",
    "            # asks for an 'Action`.\n",
    "            # We could later extend this into either\n",
    "            #   - An observation that it is making a tool-call instead of \n",
    "            #     sending an action. Turn this exception into an Observation.\n",
    "            raise NotImplementedError(\"Got tool-call in react-loop. Not implemented\")            \n",
    "        else:\n",
    "            # Assistant response\n",
    "            chat_response = response.choices[0].message.content       \n",
    "            jh.text(\n",
    "                \"<br>\".join(f\"Assistant: {chat_response}\".split('\\n')),\n",
    "                fg=\"green\")            \n",
    "            logging.info(f\"Received assistant response : {chat_response}\")\n",
    "\n",
    "            # Add response to chat response\n",
    "            chat_history.append({\n",
    "                \"role\" : \"assistant\",\n",
    "                \"content\" : chat_response\n",
    "            })\n",
    "\n",
    "            # Followup on the assistant response\n",
    "            parsed_response = ReactAssistantResponse(chat_response)\n",
    "            print(str(parsed_response))\n",
    "\n",
    "            match react_observation_from_action(parsed_response, tools):\n",
    "                case None:\n",
    "                    # Nothing added to msgs.\n",
    "                    # loop will end.\n",
    "                    logging.debug(\"🛑 React loop is terminated\")\n",
    "                case o:                    \n",
    "                    jh.text(o, fg=\"pink\", bg=\"yellow\")\n",
    "                    msgs.append({\n",
    "                        \"role\" : \"user\",\n",
    "                        \"content\": o\n",
    "                    })\n",
    "\n",
    "    \n",
    "    # return final chat_history item as the response.\n",
    "    # assert that it is from assistant ?\n",
    "    return chat_history[-1][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Assistant responses into out react semantics\n",
    "\n",
    " - create new as `ReactResponse`\n",
    " - rename to `ReactAssistantResponse`\n",
    "\n",
    "It sent the following but wrapped in triple quotes. I can't show those quotes in markdown since it is markdown code-block start/end.\n",
    "\n",
    "```\n",
    "Thought: I need to calculate the expression step by step.\n",
    "Action: functions.multiply\n",
    "Action Input: {\"a\": 2, \"b\": 4}\n",
    "```\n",
    "\n",
    " - ❓`functions.` where is this coming from ? I can add it to the regex for sure, but it is unexpected\n",
    " - the leading triple-quotes. Was that taken from the exemplars in the system prompt ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have 3 matches\n",
      "Key = Thought\n",
      "Value = I need to calculate the expression step by step.\n",
      "Key = Action\n",
      "Value = functions.multiply\n",
      "Key = Action Input\n",
      "Value = {\"a\": 2, \"b\": 4}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "----"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply\n"
     ]
    }
   ],
   "source": [
    "# First assistant response to \"What is 20+(2*4)? Calculate step by step\"\n",
    "resp = \"\"\"```\n",
    "Thought: I need to calculate the expression step by step.\n",
    "Action: functions.multiply\n",
    "Action Input: {\"a\": 2, \"b\": 4}\n",
    "```\"\"\".strip()\n",
    "\n",
    "import re\n",
    "\n",
    "pat_th = re.compile(r\"^(Thought|Action|Action Input)\\s*:\\s*(.*?)$\", re.MULTILINE)\n",
    "match_list = pat_th.findall(resp)#\"Thought: I need to calculate the expression step by step.\")\n",
    "if match_list or len(match_list) > 0:    \n",
    "    print(f\"Have {len(match_list)} matches\")\n",
    "    for m in match_list:    \n",
    "        print(f\"Key = {m[0]}\")\n",
    "        print(f\"Value = {m[1]}\")\n",
    "else:\n",
    "    print(f\"Not a valid (Thought/Action/Action Input) response\")\n",
    "\n",
    "DM.hr()\n",
    "\n",
    "pat_act = re.compile(r\"^\\s*(?:function[s]?\\.)?(.*)$\")\n",
    "m_act   = pat_act.match(\" functions.multiply\")\n",
    "if m_act:\n",
    "    print(m_act.group(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:34:13 DEBUG:Extracted [Thought = I need to calculate the expression step by step.] pair\n",
      "02:34:13 DEBUG:Extracted [Action = functions.multiply] pair\n",
      "02:34:13 DEBUG:Extracted [Action Input = {\"a\": 2, \"b\": 4}] pair\n",
      "02:34:13 DEBUG:Got Action = multiply\n",
      "02:34:13 DEBUG:Got Action Input = {\"a\": 2, \"b\": 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought     : I need to calculate the expression step by step.\n",
      "Action      : multiply\n",
      "Action Input: {\"a\": 2, \"b\": 4}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import logging\n",
    "\n",
    "class ReactAssistantResponse:\n",
    "    PATTERN_TH        = re.compile(r\"^(Thought|Action|Action Input|Answer)\\s*:\\s*(.*?)$\", re.MULTILINE)\n",
    "    PATTERN_FUNC_NAME = re.compile(r\"^\\s*(?:function[s]?\\.)?(.*)$\")\n",
    "\n",
    "    def __init__(self, assistant_response:str):\n",
    "        self.thought      = None\n",
    "        self.action       = None\n",
    "        self.action_input = None\n",
    "        self.answer       = None\n",
    "\n",
    "        #-- Parse -----------------        \n",
    "        match_list = self.PATTERN_TH.findall(assistant_response)\n",
    "\n",
    "        if match_list or len(match_list) > 0:\n",
    "            d = {}\n",
    "            for m in match_list:\n",
    "                key = m[0]\n",
    "                val = m[1]\n",
    "                logging.debug(f\"Extracted [{key} = {val}] pair\")\n",
    "                d[key] = val\n",
    "            self._init_kvps(d)\n",
    "        else:\n",
    "            logging.debug(\"Could not extract any exected React semantic sections from assistant response\\n{assistant_response}\")\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"\"\"\n",
    "Thought     : {self.thought}\n",
    "Action      : {self.action}\n",
    "Action Input: {self.action_input}\"\"\".strip()\n",
    "\n",
    "    def _init_kvps(self, d):\n",
    "        for k,v in d.items():\n",
    "            match k.lower():\n",
    "                case \"thought\":\n",
    "                    self.thought = v.strip()\n",
    "                case \"action\":\n",
    "                    # These seem to sometimes comes in as `function.my_action`\n",
    "                    fn_match = self.PATTERN_FUNC_NAME.match(v)\n",
    "                    if fn_match:\n",
    "                        self.action = fn_match.group(1)\n",
    "                        logging.debug(f\"Got Action = {self.action}\")\n",
    "                    else:\n",
    "                        logging.error(f\"Unable to get function name from Action=\\\"{v}\\\"\")\n",
    "                case \"action input\":\n",
    "                    self.action_input = v\n",
    "                    logging.debug(f\"Got Action Input = {v}\")\n",
    "                case \"answer\":\n",
    "                    self.answer = v\n",
    "                    logging.debug(f\"Got Answer = {v}\")\n",
    "                case _:\n",
    "                    logging.warning(f\"Unknown Key={k} with Value={v}\")\n",
    "\n",
    "#-- test -----------------------\n",
    "# # First assistant response to \"What is 20+(2*4)? Calculate step by step\"\n",
    "resp = \"\"\"```\n",
    "Thought: I need to calculate the expression step by step.\n",
    "Action: functions.multiply\n",
    "Action Input: {\"a\": 2, \"b\": 4}\n",
    "```\"\"\".strip()\n",
    "\n",
    "rr = ReactAssistantResponse(resp)\n",
    "print(str(rr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
